[{"path":"index.html","id":"introduction","chapter":"Introduction","heading":"Introduction","text":"Bayesian statistics can found almost everywhere science. example, epidemiology predict spread viruses, ecology understand extinction plant animal species, computer science filter unwanted emails. widespread adoption Bayesian methods past decades largely driven advances computing power. also due nature approach , closely matches learn, reason, accumulate knowledge.book, offer introduction Bayesian statistics. currently reading electronic version book, English version book published Quae March 2026.goals writing book twofold:\n1) synthesize key methodological concepts essential understand, \n2) provide practical tools can apply Bayesian statistics .learn best , use software practice statistics. software R, free open-source environment statistical computing data science.Bayesian analysis specifically, present two practical tools:brms, provides simple familiar syntax similar classical regression modeling R;NIMBLE, requires programming offers great flexibility.Rather adopting formal academic style, chose write book room - video call - explaining Bayesian statistics directly. result, occasionally (sometimes often) use informal language mathematical shortcuts make ideas easier grasp. hope mind.","code":""},{"path":"index.html","id":"why-learn-bayesian-statistics","chapter":"Introduction","heading":"Why learn Bayesian statistics?","text":"Bayesian statistics provides framework analyzing data making decisions uncertainty, much like predicting weather rolling die: know exactly happen, can estimate probability different outcomes.adopt approach? Several reasons may motivate use:natural interpretation probability: Bayesian statistics, probability represents degree belief hypothesis parameter, aligning well intuitively reason uncertainty;Great flexibility: Bayesian framework handles incomplete, heterogeneous, scarce data, well complex models (hierarchical, nonlinear, dynamic, etc.);Integration prior knowledge: previous studies expert knowledge can incorporated transparently formally;Explicit uncertainty quantification: Bayesian inference provides parameter estimates also direct measures uncertainty.","code":""},{"path":"index.html","id":"what-you-will-learn-in-this-book","chapter":"Introduction","heading":"What you will learn in this book","text":"goal guide learning process Bayesian statistics. gathered material consider essential understanding applying approach. end, feel comfortable using Bayesian methods data.objectives :demystify Bayesian statistics Markov chain Monte Carlo (MCMC) methods;understand differences Bayesian frequentist approaches;read interpret “methods” sections scientific articles using Bayesian analysis;implement Bayesian analyses R.Chapter 1 introduces foundations, revisiting key probability concepts presenting core ideas simple example.Chapter 2 takes behind scenes Bayesian inference, explaining MCMC methods guiding coding Bayesian analysis.Chapter 3 introduces two powerful tools Bayesian modeling: NIMBLE brms.Chapter 4 focuses prior distributions—choose , incorporate existing knowledge, avoid common pitfalls.Chapter 5 presents Bayesian linear regression, including model comparison validation, examples using NIMBLE brms.Chapter 6 extends discussion generalized linear models, without random effects, illustrated simulated data.Finally, last chapter summarizes key take-home messages offers practical advice applying Bayesian statistics.","code":""},{"path":"index.html","id":"how-to-read-this-book","chapter":"Introduction","heading":"How to read this book","text":"single “best” way read book. Personally, always find difficult absorb information technical book one pass. may read sequentially dip specific sections needed.chapter, R code provided; hosted https://github.com/oliviergimenez/introduction--bayesian-statistics--R update . Practicing helps better understand check indeed understood. reading electronic version available https://oliviergimenez.github.io/introduction--bayesian-statistics--R, can copy lines code paste R run . save space avoid disrupting reading much, code shown, particular code used produce figures, available https://github.com/oliviergimenez/introduction--bayesian-statistics--R. find () complete R code texts make chapters book (following R Markdown files: index.Rmd, 01-principles.Rmd, 02-mcmcmethods.Rmd, 03-implementation.Rmd, 04-priors.Rmd, 05-regression.Rmd, 06-glms.Rmd 07-conclusions.Rmd) well (ii) R scripts cleaned text allow run code easily (compressed file scriptsR.zip).","code":""},{"path":"index.html","id":"further-reading","chapter":"Introduction","heading":"Further reading","text":"like go , recommend following books, whose list course exhaustive. books source inspiration writing book. hesitated provide references, cite (many) scientific articles, ; books sufficient.Bayesian Methods Ecology (McCarthy 2007). short truly accessible book understand apply Bayesian statistics ecology without getting lost mathematics. book website https://bit.ly/4jSlfQL.Bayesian Methods Ecology (McCarthy 2007). short truly accessible book understand apply Bayesian statistics ecology without getting lost mathematics. book website https://bit.ly/4jSlfQL.Applied Statistical Modelling Ecologists: Practical Guide Bayesian Likelihood Inference Using R, JAGS, NIMBLE, Stan TMB (Kéry Kellner 2010). practical manual learn model main Bayesian tools R (JAGS, NIMBLE, Stan TMB), based concrete ecological examples comparisons results. companion website code https://www.elsevier.com/books--journals/book-companion/9780443137150.Applied Statistical Modelling Ecologists: Practical Guide Bayesian Likelihood Inference Using R, JAGS, NIMBLE, Stan TMB (Kéry Kellner 2010). practical manual learn model main Bayesian tools R (JAGS, NIMBLE, Stan TMB), based concrete ecological examples comparisons results. companion website code https://www.elsevier.com/books--journals/book-companion/9780443137150.Bayes Rules!: Introduction Applied Bayesian Modeling (Johnson, Ott, Dogucu 2022). pedagogical book discover principles applications Bayesian statistics intuitive progressive way. book available online https://www.bayesrulesbook.com/.Bayes Rules!: Introduction Applied Bayesian Modeling (Johnson, Ott, Dogucu 2022). pedagogical book discover principles applications Bayesian statistics intuitive progressive way. book available online https://www.bayesrulesbook.com/.Bayesian Data Analysis: Tutorial R Bugs (Kruschke 2010). thorough visual tutorial guides learning Bayesian statistics step step many practical examples. Everything available https://sites.google.com/site/doingbayesiandataanalysis/.Bayesian Data Analysis: Tutorial R Bugs (Kruschke 2010). thorough visual tutorial guides learning Bayesian statistics step step many practical examples. Everything available https://sites.google.com/site/doingbayesiandataanalysis/.Bayesian Data Analysis (. Gelman et al. 2013). reference book wish acquire solid theoretical applied understanding Bayesian statistics. book website https://sites.stat.columbia.edu/gelman/book/.Bayesian Data Analysis (. Gelman et al. 2013). reference book wish acquire solid theoretical applied understanding Bayesian statistics. book website https://sites.stat.columbia.edu/gelman/book/.Statistical Rethinking: Bayesian Course Examples R Stan (McElreath 2020). captivating book learn build interpret Bayesian models first developing statistical intuition. details https://xcelab.net/rm/, highly recommend video course https://github.com/rmcelreath/stat_rethinking_2024.Statistical Rethinking: Bayesian Course Examples R Stan (McElreath 2020). captivating book learn build interpret Bayesian models first developing statistical intuition. details https://xcelab.net/rm/, highly recommend video course https://github.com/rmcelreath/stat_rethinking_2024.","code":""},{"path":"index.html","id":"how-this-book-was-written","chapter":"Introduction","heading":"How this book was written","text":"book written RStudio (http://www.rstudio.com/ide/) using bookdown package (http://bookdown.org/). website hosted via GitHub Pages (https://pages.github.com/).used R version R-4.5.2_2025-10-31 following packages:","code":""},{"path":"index.html","id":"about-the-author","chapter":"Introduction","heading":"About the author","text":"name Olivier Gimenez (https://oliviergimenez.github.io/). senior scientist CNRS. studying mathematics, completed PhD statistics applied ecology. later obtained habilitation (HDR) ecology evolution returned university study sociology.authored scientific articles (https://oliviergimenez.github.io/publication/papers/) using Bayesian statistics co-written several books (https://oliviergimenez.github.io/publication/books/), also cover Bayesian methods.can find BlueSky (oaggimenez.bsky.social) LinkedIn (olivier-gimenez-545451115/), contact email.","code":""},{"path":"index.html","id":"acknowledgements","chapter":"Introduction","heading":"Acknowledgements","text":"thank employer, French National Centre Scientific Research (CNRS). researcher meaningful valuable profession. However, witnessing gradual deterioration working conditions academia, increased competition, precarity, fewer permanent positions. fortunate work supportive environment Centre Functional Evolutionary Ecology (CEFE), collaboration collective spirit remain strong.interest Bayesian statistics dates back postdoctoral years England Scotland. thank Byron Morgan giving freedom explore field, Ruth King collaborations first experience writing book, Steve Brooks many stimulating discussions.grateful Master’s students taught ten years - unknowingly contributed shaping project. also thank students postdoctoral researchers shared part journey .Thanks people kindly agreed read parts book.Finally, book dedicated Eleni, Gabriel, Mélina.","code":""},{"path":"principles.html","id":"principles","chapter":"1 The Bayesian approach","heading":"1 The Bayesian approach","text":"","code":""},{"path":"principles.html","id":"introduction-1","chapter":"1 The Bayesian approach","heading":"1.1 Introduction","text":"chapter, lay foundations revisiting probability concepts useful later . introduce key ideas Bayesian statistics simple example helps fix ideas, often use throughout book. also draw parallels classical (frequentist) statistics Bayesian statistics.","code":""},{"path":"principles.html","id":"bayes-theorem","chapter":"1 The Bayesian approach","heading":"1.2 Bayes’ theorem","text":"Let us delay get heart matter. Bayesian statistics relies Bayes’ theorem (Bayes’ formula), whose first formulation attributed mathematician Reverend Thomas Bayes. theorem published 1763, two years Bayes’ death, thanks efforts friend Richard Price. also discovered independently Pierre-Simon Laplace.Bayes’ theorem concerns conditional probabilities, can sometimes bit tricky understand. conditional probability event given event B, denoted \\(\\Pr(\\mid B)\\), probability occurs, revised taking account additional information event B occurred. example, imagine one friends rolls (fair) die asks probability result six (). answer 1/6 face die chance appearing. Now, imagine told number obtained even (B) answer. Since three even numbers, one six, can revise answer: \\(\\Pr(\\mid B) = 1/3\\).see additional information (, knowing number even) changes estimate? exactly kind reasoning Bayes’ theorem formalizes generalizes: makes possible compute probability event given another event B occurred. precisely, Bayes’ theorem gives \\(\\Pr(\\mid B)\\) using marginal probabilities \\(\\Pr()\\) \\(\\Pr(B)\\) probability \\(\\Pr(B \\mid )\\):\\[\\Pr(\\mid B) = \\displaystyle{\\frac{ \\Pr(B \\mid ) \\; \\Pr()}{\\Pr(B)}}.\\]talk marginal probability interested probability event “”, without particular condition. example, \\(\\Pr()\\) \\(\\Pr(B)\\) overall chances B, without taking anything else account. say “marginal” , made table possible combinations (instance, die outcomes classified even/odd “six/six”), \\(\\Pr()\\) \\(\\Pr(B)\\) obtained adding cells row column—.e., read margin table.Bayes’ theorem often seen way go effect B back unknown cause , knowing probability effect B given cause . Think, example, situation medical diagnosis needed, unknown disease B symptoms; physician knows risks certain symptoms depending several diseases, .e. \\(\\Pr(\\text{symptoms}|\\text{disease})\\), wishes infer probability disease given symptoms, .e. \\(\\Pr(\\text{disease}|\\text{symptoms})\\). way “reversing” \\(\\Pr(B \\mid )\\) \\(\\Pr(\\mid B)\\) Bayesian reasoning sometimes called “inverse probability”.Rather using letters risk getting confused, find easier remember Bayes’ theorem written like :\\[\\Pr(\\text{hypothesis} \\mid \\text{data}) = \\frac{ \\Pr(\\text{data} \\mid \\text{hypothesis}) \\; \\Pr(\\text{hypothesis})}{\\Pr(\\text{data})}.\\]hypothesis can parameter probability disease occurs, regression coefficients linking probability risk factors (example, place residence, smoking). Bayes’ theorem tells us obtain probability hypothesis available data.\nrelevant , think : exactly scientific method . want know plausible hypothesis given data collected, perhaps compare several hypotheses one another. point view, Bayesian reasoning aligns scientific reasoning, probably explains Bayesian framework feels natural understanding statistics.might ask Bayesian statistics norm. long time, implementing Bayes’ theorem limited computational difficulties, see next chapter. Fortunately, increases computing power development new algorithms led marked rise Bayesian approach past thirty years.","code":""},{"path":"principles.html","id":"statbayes","chapter":"1 The Bayesian approach","heading":"1.3 What is Bayesian statistics?","text":"Typical statistical problems consist estimating one (several) parameters available data. Let us denote parameter (parameters) generically, say \\(\\theta\\). estimate \\(theta\\), probably familiar frequentist approach Bayesian approach. frequentist approach, particular maximum likelihood estimation, assumes parameters fixed unknown. Classical estimators therefore generally point values; instance, estimator probability obtaining face die number times face observed divided number times die rolled. Bayesian approach assumes parameters fixed follow unknown distribution. probability distribution mathematical expression gives probability random variable takes certain values. can discrete (example, Bernoulli distribution, binomial distribution, Poisson distribution) continuous (normal Gaussian distribution).Bayesian approach rests idea start knowledge system even studying . , collect data update prior knowledge based observations. updating process relies Bayes’ theorem. simplified form, taking \\(= \\theta\\) \\(B = \\text{data}\\), Bayes’ theorem makes possible estimate parameter \\(\\theta\\) data follows:\\[\\Pr(\\theta \\mid \\text{data}) = \\frac{\\Pr(\\text{data} \\mid \\theta) \\times \\Pr(\\theta)}{\\Pr(\\text{data})}.\\]Let us take moment review term formula.left, \\(\\Pr(\\theta \\mid \\text{data})\\), posterior distribution: probability \\(\\theta\\) given data. represents know \\(\\theta\\) seeing data. basis inference precisely looking : distribution, possibly multivariate several parameters.right, \\(\\Pr(\\text{data} \\mid \\theta)\\), likelihood. probability data given \\(\\theta\\). quantity classical frequentist approach. Yes: Bayesian frequentist approaches share component, likelihood, explains results often close. likelihood expresses information contained data, given model parameterized \\(\\theta\\). come back Section 1.5.Next, \\(\\Pr(\\theta)\\), prior distribution. quantity represents know \\(\\theta\\) seeing data. prior distribution depend data; words, one use data construct . can vague non-informative know nothing \\(\\theta\\). Often, never really start zero, ideally like prior reflect existing knowledge. discuss priors detail Chapter 4.Finally, denominator \\(\\Pr(\\text{data})\\), sometimes called average likelihood, averaged respect prior, obtained integrating likelihood prior distribution:\n\\({\\Pr(\\text{data}) = \\int{\\Pr(\\text{data} \\mid \\theta) \\times \\Pr(\\theta) \\, d\\theta}}\\).\nquantity normalizes posterior distribution integrates 1. words, since \\(\\int{\\Pr(\\theta \\mid \\text{data}) \\, d\\theta} = 1\\) integral probability density equals 1, \n\\(\\displaystyle \\int{\\frac{\\Pr(\\text{data} \\mid \\theta) \\times \\Pr(\\theta)}{\\Pr(\\text{data})} \\, d\\theta } = 1\\).\nsince \\(\\Pr(\\text{data})\\) depend \\(\\theta\\), \n\\(\\Pr(\\text{data}) = \\int{\\Pr(\\text{data} \\mid \\theta) \\times \\Pr(\\theta) \\, d\\theta}\\).\nintegral whose dimension equals number parameters \\(\\text{theta}\\) estimate: two parameters, double integral; three parameters, triple integral; . However, beyond three dimensions, becomes difficult, even impossible, compute integral. one reasons Bayesian approach used earlier, need algorithms estimate posterior distributions, explain Chapter 2. meantime, work relatively simple example posterior distribution explicit form.","code":""},{"path":"principles.html","id":"a-running-example","chapter":"1 The Bayesian approach","heading":"1.4 A running example","text":"Let us take concrete example fix ideas. work coypu (Myocastor coypus) (Figure 1.1), semi-aquatic rodent native South America, introduced Europe fur farming. now considered invasive alien species, damage causes wetlands (bank erosion, destruction vegetation) possible role transmitting leptospirosis humans, potentially severe bacterial infection transmitted water. Thanks high fecundity good adaptation temperate climates, coypu proliferated rapidly.\nFigure 1.1: Photograph coypus (Myocastor coypus) taken Lez watershed near Montpellier, France. Credits: Yann Raulet.\nOne questions interested estimating probability surviving winter, coypus particularly sensitive cold. , equip several individuals GPS tag beginning winter, say \\(n = 57\\). end winter, observe \\(y = 19\\) coypus still alive. goal estimate winter survival probability, denote \\(\\theta\\). data:probably thinking , information, can already estimate survival probability. Intuitively, think proportion individuals survived, .e. \\(19/57\\). wrong. reasonable estimate \\(\\theta\\), winter survival probability. Let us now try formalize intuition, order better understand represents, assumes.mentioned , likelihood central concept found frequentist Bayesian approaches. let us start constructing likelihood. , need make assumptions.First, assume individuals independent, meaning survival one coypu influence survival coypus. strong assumption, especially know female can reproduce two three times per year give birth ten offspring depend early life. , modeling, often better start simple.Second, assume individuals survival probability. , simplification: know, example, juvenile mortality higher adult mortality.two assumptions, number \\(y\\) animals still alive end winter follows binomial distribution, \\(\\theta\\) probability success (survival) \\(n\\) number trials (monitored individuals). write \\(y \\sim \\text{Bin}(n, \\theta)\\). binomial distribution fact sum several independent Bernoulli trials, classic heads--tails example. trial—, release GPS-tagged coypu beginning winter—assume probability \\(\\theta\\) success, .e. surviving winter, failure, .e. dying cold. trials independent probability success (assumptions), number successes, number coypus alive end winter, follows binomial distribution (see also Chapter 6). provide examples Bernoulli binomial draws Figure 1.2.\nFigure 1.2: Discrete probability distributions, Bernoulli binomial, illustrated 100 simulations (random draws generated computer). top row, show observed frequency Bernoulli draw different values survival probability \\(\\theta\\). bottom row, show histograms binomial draw 50 trials different values survival probability \\(\\theta\\).\naside, easy get mixed terms used describe Bernoulli binomial distribution (normal distribution): can remember probability number, distribution law, density function represents .","code":"\ny <- 19 # number of individuals that survived the winter\nn <- 57 # number of individuals monitored at the start of winter"},{"path":"principles.html","id":"maxvrais","chapter":"1 The Bayesian approach","heading":"1.5 Maximum likelihood","text":"classical (frequentist) approach, estimate survival probability \\(\\theta\\) using maximum likelihood method. mean practice? means finding value \\(\\theta\\) makes observed data likely. words, since data —observed—look value \\(\\theta\\) maximizes probability dataset generated.justify rather intuitive idea mathematically? Read carefully end previous paragraph. idea looking value gives largest probability amounts maximizing something. exactly? probability data, given certain model parameterized \\(\\theta\\)—words, likelihood, \\(\\Pr(\\text{data}|\\theta)\\), saw Section 1.3. Classical estimation therefore relies maximizing likelihood—rather likelihood function, .e. likelihood considered function \\(\\theta\\).case, binomial experiment: follow \\(n\\) coypus winter, probability \\(\\theta\\) surviving. know probability possible outcome (probability mass function). example, probability coypu survives \\((1-\\theta)^n\\), \\(n\\) individuals dies probability \\(1-\\text{theta}\\). take, example, survival probability 0.5, \\((1-0.5)^{57} \\approx 0\\). can compute probability R dbinom() function:first argument x = 0 corresponds coypu alive. Conversely, probability survive \\(\\theta^n\\), value. can check R dbinom(x = 57, size = 57, prob = 0.5). exactly one coypu survives, one \\(n\\) survives probability \\(\\theta\\), \\(n-1\\) die probability \\((1-\\theta)^{n-1}\\). Since \\(n\\) coypus can one survives, obtain total probability \\(n,\\theta,(1-\\theta)^{n-1}\\). can compute probability dbinom(x = 1, size = 57, prob = 0.5). generally, probability \\(y\\) individuals survive given \\(\\displaystyle \\binom{n}{y}\\theta^y(1-\\theta)^{n-y}\\). consider expression function \\(\\theta\\) (\\(y\\)), obtain likelihood function \\(\\displaystyle \\mathcal{L}(\\theta) = \\binom{n}{y} \\theta^y (1 - \\theta)^{n - y}\\). term \\(\\displaystyle \\binom{n}{y}\\) called binomial coefficient read “\\(y\\) \\(n\\)”. corresponds number different ways choose \\(y\\) survivors among \\(n\\) coypus, without regard order.can plot likelihood R Figure 1.3:\nFigure 1.3: Likelihood function winter survival probability coypu, computed \\(y=19\\) survivors \\(n=57\\) individuals monitored GPS. maximum likelihood estimate indicated red dashed line.\ngoal find value \\(\\theta\\) maximizes function. words, look survival value (x-axis Figure 1.3) maximizes likelihood (y-axis). value corresponds maximum likelihood estimator, often denoted \\(\\hat{\\theta}\\). , often convenient work logarithm likelihood (log-likelihood), sums numerically stable easier differentiate products:\\[\n\\ell(\\theta) = \\log \\mathcal{L}(\\theta) = \\log \\binom{n}{y} + y \\log \\theta + (n - y) \\log (1 - \\theta).\n\\]\nfirst term, \\(\\displaystyle \\log \\binom{n}{y}\\), depend \\(\\theta\\), can ignore follows. differentiate log-likelihood respect \\(\\text{theta}\\):\\[\n\\displaystyle \\frac{d\\ell(\\theta)}{d\\theta} = \\frac{y}{\\theta} - \\frac{n - y}{1 - \\theta}.\n\\]look value \\(\\theta\\) makes derivative equal zero:\\[\n\\frac{y}{\\theta} - \\frac{n - y}{1 - \\theta} = 0.\n\\]simplifications, obtain maximum likelihood estimator \\(\\hat{\\theta}\\) :\\[\n\\hat{\\theta} = \\frac{y}{n}.\n\\]result matches initial intuition: maximum likelihood estimator proportion individuals survived, .e. \\(19/57 \\approx 0.333\\). can visualize result Figure 1.3, maximum likelihood estimate indicated red dashed line.practice, models contain multiple parameters—dozens even hundreds—apply analytic method maximize likelihood find maximum likelihood estimators. Instead, use iterative optimization algorithms solve problem us, adjusting step step initial value find one maximizes likelihood. example, R, can obtain exactly result using logistic regression without covariates (see Chapter 6):direct calculation \\(\\hat{\\theta}=y/n\\) result calling glm function consistent: give value.","code":"\ndbinom(x = 0, size = 57, prob = 1 - 0.5)\n#> [1] 6.938894e-18\nmod <- glm(cbind(y, n - y) ~ 1, family = binomial)\ntheta_hat <- plogis(coef(mod))\ntheta_hat\n#> (Intercept) \n#>   0.3333333"},{"path":"principles.html","id":"and-in-the-bayesian-framework","chapter":"1 The Bayesian approach","heading":"1.6 And in the Bayesian framework?","text":"Bayesian approach, start expressing prior knowledge quantity want estimate—, winter survival probability theta. know theta continuous variable 0 1. natural prior distribution case beta distribution. beta distribution defined two parameters, \\(\\) \\(b\\), control shape:\\[\nq(\\theta \\mid , b) = \\frac{1}{\\text{Beta}(, b)}{\\theta^{- 1}} {(1-\\theta)^{b - 1}}\n\\]:\\[\n\\text{Beta}(, b) = \\frac{\\Gamma()\\Gamma(b)}{\\Gamma(+b)}, \\quad \\Gamma(n) = (n-1)!\n\\]can forget equations comfortable . Let us instead visualize distribution Figure 1.4:\nFigure 1.4: Examples beta distributions different values parameters \\(\\) \\(b\\). panel, shaded areas illustrate probability observing value within given interval.\npanel figure shows shape beta distribution given pair parameters \\((, b)\\). Several characteristic behaviors can observed.Beta(1,1) (top left) corresponds uniform distribution 0 1: values theta 0 1 considered equally likely. density constant, means probability observing value 0.1 0.2 observing one 0.8 0.9. probability area rectangle bounded red curve vertical lines 0.1 0.2 (0.8 0.9), .e. red shaded areas. corresponds situation prior knowledge.Beta(2,1) Beta(1,2) represent asymmetric knowledge: former biased toward values close 1, latter toward values close 0. probability observing value 0.1 0.2 smaller observing one 0.8 0.9, vice versa.Beta(2,2) symmetric puts weight central values uniform distribution. probability observing value 0.1 0.2 smaller observing one 0.5 0.6.Beta(10,10) represents knowledge highly concentrated around 0.5: informative prior. probability observing value 0.2 0.3 much smaller observing one 0.5 0.6.Beta(0.8,0.8) illustrates U-shaped (bathtub-shaped) distribution favors extreme values (close 0 1). probabilities observing value 0 0.1 0.9 1 larger observing one 0.45 0.55.examples make possible visualize parameters \\(\\) \\(b\\) influence shape prior. go prior posterior distribution?assume \\(\\theta \\sim \\text{Beta}(, b)\\) observed \\(y = 19\\) survivors among \\(n = 57\\) individuals. likelihood \\(\\displaystyle \\binom{n}{y}\\theta^y(1 - \\theta)^{n - y}\\). now, ignore denominator \\(\\Pr(y)\\) Bayes’ theorem; see next chapter . Thus, posterior proportional product likelihood prior:\n\\(\\Pr(\\theta \\mid y) \\propto \\Pr(y \\mid \\theta) \\times \\Pr(\\theta)\\).\ncase, multiply likelihood prior term term, rearranging terms \\(\\theta\\) \\(1-\\theta\\), obtain:\\[\n\\begin{aligned}\n\\Pr(\\theta \\mid y) &\\propto \\underbrace{\\theta^y (1 - \\theta)^{n - y}}_{\\text{binomial likelihood}} \\times \\underbrace{\\theta^{- 1} (1 - \\theta)^{b - 1}}_{\\text{beta prior}} \\\\\n&\\propto \\underbrace{\\theta^{+ y - 1} (1 - \\theta)^{b + n - y - 1}}_{\\text{yet another beta distribution}}\n\\end{aligned}\n\\]words, obtain beta distribution, updated parameters \\(+ y\\) \\(b + n - y\\). say binomial beta distributions conjugate: use beta distribution prior probability parameter binomial model, resulting posterior distribution also beta distribution. use uniform prior 0 1 (.e. Beta(1,1)), obtain posterior distribution winter survival \n\\(\\text{Beta}(1+19, 1+57-19) = \\text{Beta}(20, 39)\\).\nMoreover, posterior distribution known, greatly facilitates computations interpretation. example, know mean \\(\\text{Beta}(, b)\\) \\(\\displaystyle \\frac{}{+b}\\), .e. \\(\\frac{20}{59} \\approx 0.339\\). can compare value maximum likelihood estimator \\(19/57 \\approx 0.333\\). can also visualize posterior distribution Figure (ref?)(fig:posterior-survie), since know equation beta density:\nFigure 1.5: Uniform prior (red) posterior distribution (black) coypu winter survival probability. blue dashed line corresponds maximum likelihood estimate.\ngenerally, enough data, Bayesian frequentist estimators tend close. Intuitively, data end “dominating” prior information. Roughly speaking, mode posterior distribution (value density maximal) corresponds exactly maximum likelihood estimator.illustrates link two approaches central role likelihood statistics: fundamental common component Bayesian frequentist approaches.","code":""},{"path":"principles.html","id":"in-summary","chapter":"1 The Bayesian approach","heading":"1.7 In summary","text":"Bayes’ theorem tool updating knowledge.Bayesian statistics relies likelihood prior distribution model parameters.Frequentist statistics provides point estimator, whereas Bayesian statistics estimates distribution parameter.Often, classical Bayesian approaches yield similar estimates.cases, posterior distribution explicit (example, case beta/binomial conjugacy).cases, need use simulations obtain posterior distribution, see Chapter 2.","code":""},{"path":"mcmc.html","id":"mcmc","chapter":"2 MCMC methods","heading":"2 MCMC methods","text":"","code":""},{"path":"mcmc.html","id":"introduction-2","chapter":"2 MCMC methods","heading":"2.1 Introduction","text":"hope lose (much) previous chapter equations. new chapter, go behind scenes Bayesian statistics introducing Markov chain Monte Carlo (MCMC) methods. see simulation techniques become essential implementing Bayesian inference practice. nothing beats practice, get hands little dirty coding , using running example estimating survival probability.","code":""},{"path":"mcmc.html","id":"applying-bayes-theorem","chapter":"2 MCMC methods","heading":"2.2 Applying Bayes’ theorem","text":"Let us return running example coypus; repeat data:Let us apply Bayes’ theorem directly Chapter 1, set aside denominator \\(\\Pr(\\text{data})\\). Let us see whether can handle . saw, denominator given \n\\(\\displaystyle \\Pr(\\text{y}) = \\int{\\Pr(\\text{data} \\mid \\theta) \\Pr(\\theta) \\, d\\theta}\\).\ncompute integral. Let us start writing R function computes product (binomial) likelihood prior (Beta(1,1)), .e. numerator Bayes’ theorem, \\(\\Pr(\\text{data} \\mid \\theta) \\times \\Pr(\\theta)\\):can now write function computes denominator. , use R’s integrate() function, computes integral one-variable function. integrate() function uses quadrature techniques approximate area curve defined function integrate, breaking small pieces summing .obtain numerical approximation posterior distribution winter survival, Figure 2.1:\nFigure 2.1: Numerical approximation posterior distribution winter survival.\ngood numerical approximation? Ideally, like compare approximation true posterior distribution. Conveniently, obtained Chapter 1: beta distribution parameters 20 39. Figure 2.2, can see two curves overlap perfectly.\nFigure 2.2: Comparison exact posterior (brick red) numerical approximation (cream).\nexact posterior distribution (brick red) numerical approximation (cream) winter survival indistinguishable, suggesting numerical approximation satisfactory.example, single parameter estimate: winter survival. means denominator involves one-dimensional integral, fairly easy handle quadrature techniques R’s integrate() function.happens several parameters? example, imagine want fit regression model survival depends explanatory variable, say coypu body mass. effect variable captured regression parameters \\(\\beta_0\\) (intercept) \\(\\beta_1\\) (slope), also residual error standard deviation \\(\\sigma\\) (see Chapter 5). Bayes’ theorem gives joint posterior distribution parameters (.e. three parameters together):\\[ \\displaystyle \\Pr(\\beta_0, \\beta_1, \\sigma \\mid \\text{y}) = \\frac{ \\Pr(\\text{y} \\mid \\beta_0, \\beta_1, \\sigma) \\times \\Pr(\\beta_0, \\beta_1, \\sigma)}{\\displaystyle \\iiint \\Pr(\\text{y} \\mid \\beta_0, \\beta_1, \\sigma) \\Pr(\\beta_0, \\beta_1, \\sigma) \\, d\\beta_0 \\, d\\beta_1 \\, d\\sigma} \\]two major numerical challenges:really want compute triple integral? , classical methods rarely go much beyond two dimensions.often interested marginal distributions parameters (example, \\(\\beta_1\\), effect mass survival), obtained integrating joint posterior distribution parameters (, double integral respect \\(\\beta_0\\) \\(\\sigma\\)) — quickly becomes intractable number parameters increases.next section, introduce powerful simulation methods overcome limitations.","code":"\ny <- 19 # number of individuals that survived the winter\nn <- 57 # number of individuals monitored at the start of winter\nnum <- function(theta) dbinom(y, n, theta) * dbeta(theta, 1, 1)\nden <- integrate(num, 0, 1)$value\n# Create a grid of possible values for the survival probability (between 0 and 1)\ngrid <- seq(0, 1, 0.01)\n\n# Compute posterior density values on the grid\n# num(grid) is likelihood * prior, and den is the normalizing constant\nposterior <- data.frame(\n  survival = grid,\n  ratio = num(grid) / den  # normalized posterior density\n)\n\n# Plot the posterior density curve\nposterior %>%\n  ggplot(aes(x = survival, y = ratio)) +\n  geom_line(size = 1.5) +\n  labs(x = \"Survival probability\", y = \"Density\") +\n  theme_minimal()"},{"path":"mcmc.html","id":"mcmc-algorithms","chapter":"2 MCMC methods","heading":"2.3 MCMC algorithms","text":"short, idea Markov chain Monte Carlo (MCMC) methods use simulations approximate posterior distributions given precision drawing large number samples. avoids explicit computation multidimensional integrals arise applying Bayes’ theorem.simulation algorithms consist two parts: Markov chains Monte Carlo. Let us try understand two terms.Monte Carlo mean? Monte Carlo integration simulation technique used compute integrals arbitrary functions \\(f\\) random variable \\(X\\) distribution \\(\\Pr(X)\\), \\(\\displaystyle \\int f(X) \\Pr(X) dX\\). draw values \\(X_1, \\ldots, X_k\\) \\(\\Pr(X)\\), apply function \\(f\\) values, compute mean resulting values, \\(\\displaystyle{\\frac{1}{k}}\\sum_{=1}^k{f(X_i)}\\), approximate integral.use Monte Carlo integration Bayesian context? posterior distribution contains information need parameter(s) want estimate. multiple parameters, often want summarize information computing numerical summaries. simplest summary posterior mean,\n\\(E(\\theta) = \\int \\theta \\Pr(\\theta \\mid \\text{data}) \\, d\\theta\\),\n\\(X\\) \\(\\theta\\) \\(f\\) identity. posterior mean can estimated Monte Carlo integration; example, coypu survival:can verify resulting mean close theoretical expectation beta distribution:Another useful numerical summary credible interval within parameter lies given probability, usually 0.95, .e. 95% credible interval. Determining bounds interval requires computing quantiles, also relies integrals, therefore Monte Carlo integration. 95% credible interval winter survival can obtained :way, difference credible interval Bayesian statistics confidence interval frequentist statistics. 95% confidence interval means repeated experiment large number times (tag coypus GPS record number winter survivors), 95% intervals constructed way contain true parameter value \\(\\theta\\). say probability parameter lies within given interval 95%. 95% credible interval, contrast, means 95% probability parameter lies within interval. interpretation credible interval bit intuitive confidence interval.Now, Markov chain? Markov chain random sequence numbers number depends previous one. One example weather city, Montpellier, south France, sunny day likely followed another sunny day, say probability 0.8, rainy day rarely followed another rainy day, say probability 0.1. dynamics Markov chain captured transition matrix:\\[\n\\begin{array}{c|cc}\n& \\text{Sunny tomorrow} & \\text{Rainy tomorrow} \\\\ \\\\ \\hline\n\\text{Sunny today} & 0.8 & 0.2 \\\\\\\\\n\\text{Rainy today}   & 0.9 & 0.1\n\\end{array}\n\\]Rows indicate today’s weather columns indicate tomorrow’s. cells give probability sunny rainy day tomorrow depending today’s weather (conditional probabilities; see Chapter 1).certain conditions, Markov chain converges unique stationary distribution. weather example, let us iterate chain 20 steps:row matrix converges toward distribution \\((0.82, 0.18)\\) number steps increases. convergence occurs regardless starting state: probability 0.82 sun 0.18 rain.Let us return MCMC methods. central idea can construct Markov chain whose stationary distribution precisely posterior distribution parameters. Keep idea mind: fundamental.combining Monte Carlo Markov chains, MCMC methods allow us generate sample values whose distribution converges posterior distribution (Markov chain) use sample compute posterior numerical summaries (Monte Carlo), mean credible intervals.several ways build Markov chains Bayesian inference. may heard Metropolis–Hastings algorithm Gibbs sampler. can consult https://chi-feng.github.io/mcmc-demo/ interactive gallery MCMC algorithms. , illustrate Metropolis algorithm practical implementation. draw inspiration excellent book Jim Albert (2009). goal able write algorithm scratch, grasp main ideas , , notion simulation.Let us return survival example. illustrate sampling posterior distribution survival. Let us start writing functions likelihood, prior, posterior. work log scale manipulate sums differences rather products ratios, can make numerical calculations unstable:Metropolis algorithm works follows:Choose initial value parameter estimate. starting value, initial point Markov chain.Choose initial value parameter estimate. starting value, initial point Markov chain.decide next step, propose moving away current parameter value—candidate value. add current value draw normal distribution variance—proposal distribution. Metropolis algorithm special case Metropolis–Hastings symmetric proposals.decide next step, propose moving away current parameter value—candidate value. add current value draw normal distribution variance—proposal distribution. Metropolis algorithm special case Metropolis–Hastings symmetric proposals.Compute ratio posterior densities candidate position current position:\n\\(R = \\displaystyle \\frac{\\Pr(\\text{candidate value}|\\text{data})}{\\Pr(\\text{current value}|\\text{data})}\\).\ncompute numerator denominator, simply apply Bayes’ theorem, magic MCMC happens: \\(\\Pr(\\text{data})\\) appears numerator denominator, cancels, longer need compute . replaced computation integral simulations.Compute ratio posterior densities candidate position current position:\n\\(R = \\displaystyle \\frac{\\Pr(\\text{candidate value}|\\text{data})}{\\Pr(\\text{current value}|\\text{data})}\\).\ncompute numerator denominator, simply apply Bayes’ theorem, magic MCMC happens: \\(\\Pr(\\text{data})\\) appears numerator denominator, cancels, longer need compute . replaced computation integral simulations.posterior density candidate position larger current position, .e. candidate value plausible, accept immediately. Otherwise, accept probability \\(R\\), reject probability \\(1 - R\\). example, candidate value ten times less plausible, accept probability 0.1. use uniform random number 0 1 (call \\(X\\)): \\(X < R\\), accept candidate value; otherwise, stay current value. practice, aim acceptance rate 0.2 0.4, can adjusted calibrating proposal variance; helps explore whole parameter space.posterior density candidate position larger current position, .e. candidate value plausible, accept immediately. Otherwise, accept probability \\(R\\), reject probability \\(1 - R\\). example, candidate value ten times less plausible, accept probability 0.1. use uniform random number 0 1 (call \\(X\\)): \\(X < R\\), accept candidate value; otherwise, stay current value. practice, aim acceptance rate 0.2 0.4, can adjusted calibrating proposal variance; helps explore whole parameter space.Repeat steps 2 4 certain number times—iterations.Repeat steps 2 4 certain number times—iterations.Enough theory: let us implement . start initializing:need initialize? running Markov chain, prepare objects store simulated values parameter (, survival probability) well information whether proposal accepted. set.seed(666) ? command sets seed random number generator. ensures simulations reproducible: rerun code, obtain exactly simulated values mine.choose starting value:starting value? Markov chain start somewhere: , arbitrarily choose 0.5 initial value survival probability. constraint value must compatible prior: going pick negative survival probability 15. place value first element theta.post, indicate accept[1] <- 1 first value accepted construction, since starting point.Next, write function propose candidate value current value:function introduces random proposal around current value. work logit scale ensure final proposal (candidate) always remains interval (0,1) (see also Chapter 6). away parameter controls spread proposals: larger , larger jumps; smaller , closer proposals remain current value.implement steps 2 4 algorithm loop (step 5: repeating iterations):loop builds Markov chain iteratively. probability accepting less plausible value proportional likelihood ratio. accept vector can used diagnose acceptance frequency, useful calibrating chain.Let us take look first last simulated values:can now visualize chain’s evolution trace plot, .e. curve showing simulated values theta across iterations (Figure 2.3):\nFigure 2.3: Trace plot simulated values survival probability \\(\\theta\\) across iterations.\ntrace plot tell us? horizontal axis represents iterations (“time” Markov chain). vertical axis shows simulated values survival probability step. figure, see chain sometimes stays value several consecutive iterations. happens candidate value proposed algorithm rejected—chain retains previous (precisely, current) value. times, see jumps new values, corresponding accepted proposals.can wrap algorithm reusable function, making easy run multiple chains:can now use metropolis() run another chain, time starting 0.2:Note often talk “running multiple MCMC chains” diagnose convergence. practice, independent realizations Markov chain—like flipping coin multiple times, except complicated distribution Bernoulli.plot chains together, Figure 2.4:\nFigure 2.4: Trace plot simulated values survival probability \\(\\theta\\) across iterations. Two chains run different initial values, 0.5 blue 0.2 yellow.\nNote obtain exactly results algorithm stochastic. observe parallel evolution two chains started different initial values. two chains quickly meet oscillate around values, indicates good convergence toward desired stationary distribution. key step MCMC convergence diagnostics, cover later chapter. observe convergence longer period, run chain 1,000 iterations. gives smoother trace plot showing chain stability, Figure 2.5:\nFigure 2.5: Trace plot simulated values survival probability \\(\\theta\\) across 1000 iterations.\nlarge number iterations, chain stabilize around stationary distribution. Visually, look dense, homogeneous, well-explored region—like neatly mown lawn (image).stationary distribution reached, can treat simulated values Markov chain sample posterior distribution compute numerical summaries parameters (posterior mean, credible interval).can say reached stationary distribution? convergence, many additional simulations need obtain good approximation posterior distribution parameters? address questions next section.","code":"\n# draw 1000 values from the Beta(20,39) posterior\nsample_from_posterior <- rbeta(1000, 20, 39)\n# compute the mean by Monte Carlo integration\nmean(sample_from_posterior)\n#> [1] 0.3405089\n20/(20+39) # expectation of the Beta(20,39) distribution\n#> [1] 0.3389831\nquantile(sample_from_posterior, probs = c(2.5/100, 97.5/100))\n#>      2.5%     97.5% \n#> 0.2270862 0.4702974\ntemps <- matrix(c(0.8, 0.2, 0.9, 0.1), nrow = 2, byrow = T) # transition matrix\netapes <- 20\nfor (i in 1:etapes){\n  temps <- temps %*% temps # matrix multiplication\n}\nround(temps, 2) # matrix product after 20 steps\n#>      [,1] [,2]\n#> [1,] 0.82 0.18\n#> [2,] 0.82 0.18\n# 19 animals found alive out of 57 captured, marked and released\ny <- 19\nn <- 57\n\n# binomial log-likelihood Bin(n = 57,p)\nloglikelihood <- function(x, p){\n  dbinom(x = x, size = n, prob = p, log = TRUE)\n}\n\n# uniform prior density\nlogprior <- function(p){\n  dunif(x = p, min = 0, max = 1, log = TRUE)\n  # or dbeta(x = p, shape1 = 0, shape2 = 1, log = TRUE)\n}\n\n# posterior density (log scale)\nposterior <- function(x, p){\n  loglikelihood(x, p) + logprior(p)\n}\nsteps <- 100 # number of steps (iterations) of the chain\ntheta.post <- rep(NA, steps) # vector to store simulated values\naccept <- rep(NA, steps) # vector to record accept/reject decisions\nset.seed(666) # for reproducibility\ninits <- 0.5 # chosen starting value for theta\ntheta.post[1] <- inits # record this value as the first position of the chain\naccept[1] <- 1 # the initial value is accepted by default\nmove <- function(x, away = 1){\n  logitx <- log(x / (1 - x)) # logit transform: maps x from (0,1) to (-∞,+∞)\n  logit_candidate <- logitx + rnorm(1, 0, away) # add centered normal noise, sd controlled by away\n  candidate <- plogis(logit_candidate) # inverse transform (logit^-1): returns a value between 0 and 1\n  return(candidate) # return proposed value\n}\nfor (t in 2:steps){ # for each iteration, starting at the 2nd\n\n  # Step 2: propose a new value for theta\n  theta_star <- move(theta.post[t-1])  # candidate drawn from the previous value\n\n  # Step 3: compute the ratio of posterior densities (log scale)\n  pstar <- posterior(y, p = theta_star) # posterior density at candidate\n  pprev <- posterior(y, p = theta.post[t-1]) # posterior density at current value\n  logR <- pstar - pprev # difference on the log scale\n  R <- exp(logR) # back to the natural scale (density ratio)\n\n  # Step 4: accept or reject the proposal\n  X <- runif(1, 0, 1) # random draw between 0 and 1: the acceptance \"roulette\"\n  if (X < R){ # if the proposal is more plausible (or not too much worse)\n    theta.post[t] <- theta_star # accept and store the candidate\n    accept[t] <- 1 # record acceptance\n  } else {\n    theta.post[t] <- theta.post[t-1] # otherwise keep the previous value\n    accept[t] <- 0 # record rejection\n  }\n}\nhead(theta.post)\n#> [1] 0.5000000 0.5000000 0.3021903 0.3021903 0.1853669 0.1853669\ntail(theta.post)\n#> [1] 0.4076667 0.4076667 0.4076667 0.4076667 0.2914464 0.2914464\nmetropolis <- function(steps = 100, inits = 0.5, away = 1){\n\n  theta.post <- rep(NA, steps) # vector to store samples\n  theta.post[1] <- inits # initialize with starting value\n\n  for (t in 2:steps){ # loop over steps (starting at the 2nd)\n\n    theta_star <- move(theta.post[t-1], away) # propose a new value\n\n    # log-ratio of posterior density between candidate and current value\n    logR <- posterior(y, theta_star) -\n            posterior(y, theta.post[t-1])\n    R <- exp(logR) # back to non-log scale\n\n    X <- runif(1, 0, 1) # draw a uniform random number\n    theta.post[t] <- ifelse(X < R, # if draw < acceptance probability...\n                            theta_star, # ... accept proposed value\n                            theta.post[t-1]) # otherwise keep previous\n  }\n\n  return(theta.post) # return simulated sample\n}\ntheta.post2 <- metropolis(steps = 100, inits = 0.2) # start at 0.2"},{"path":"mcmc.html","id":"convergence-diag","chapter":"2 MCMC methods","heading":"2.4 Assessing convergence","text":"applying MCMC method, need determine long takes Markov chain converge target distribution, many additional iterations required convergence obtain reliable Monte Carlo estimates numerical summaries (posterior means, credible intervals).","code":""},{"path":"mcmc.html","id":"burn-in","chapter":"2 MCMC methods","heading":"2.4.1 Burn-in","text":"practice, discard first values Markov chain use values simulated convergence. initial observations discard generally called burn-(warm-) period.simplest way determine length burn-period inspect trace plots. Let us return example look Figure 2.6, trace plot chain starting 0.99:\nFigure 2.6: Trace plot chain starting 0.99. shaded area illustrates possible burn-period.\nchain starts 0.99 stabilizes quickly, values oscillating around 0.3 iteration 100 onward. can choose shaded area burn-period discard first 100 values. safe, one use 250 even 500 iterations burn-, provided cost much computation time, course.Inspecting trace plot single chain useful, generally run multiple chains different initial values check reach stationary distribution. approach formalized Brooks–Gelman–Rubin statistic (BGR), denoted \\(\\hat{R}\\), measures ratio total variability (chains plus within chain) within-chain variability. close spirit \\(F\\) test analysis variance (, one-factor ANOVA factor levels chains). value 1.1 indicates likely convergence.Let us return example: run two Markov chains initial values 0.2 0.8, varying number iterations 100 1000 steps 50, compute BGR statistic using half iterations burn-(Figure 2.7).\nFigure 2.7: Value Brooks–Gelman–Rubin (BGR) statistic function number iterations. value close 1 suggests convergence.\nobtain BGR statistic close 1 300 iterations onward, suggesting burn-300 iterations, nothing indicates convergence problem.important remember value close 1 BGR statistic necessary sufficient condition convergence. words, diagnostic assert certainty chain converged; simply indicates detect obvious sign . advice: always take time look trace plots.","code":""},{"path":"mcmc.html","id":"chain-length","chapter":"2 MCMC methods","heading":"2.4.2 Chain length","text":"chain length needed obtain reliable parameter estimates? Keep mind successive steps Markov chain independent. called autocorrelation. Ideally, want minimize autocorrelation., trace plots can diagnose autocorrelation issues. Returning survival example, Figure 2.8 shows trace plots (3000 iterations) different values proposal normal standard deviation (parameter away) used generate candidate values.\nFigure 2.8: Trace plots different values proposal standard deviation (away). Good mixing observed away = 1. shaded gray area corresponds burn-300 iterations.\nsmall large moves visible left right panels lead strong correlation successive observations Markov chain, whereas standard deviation equal 1 (center) allows efficient exploration parameter space. movement parameter space called mixing. Mixing considered poor chain makes jumps small large, good otherwise.addition trace plots, autocorrelation function (ACF) plots provide convenient way visualize strength autocorrelation given sample. ACF plots show correlation successively sampled values separated increasing number iterations, called lag. Figure 2.9, obtain ACF plots different proposal standard deviations using forecast::ggAcf():\nFigure 2.9: Autocorrelation functions (ACF) different proposal standard deviations. Low autocorrelation sign good mixing. burn-300 iterations applied.\nleft right panels, autocorrelation strong decreases slowly lag, mixing poor. central panel, autocorrelation weak decreases quickly lag, mixing good.Autocorrelation necessarily major problem. Highly correlated observations simply require larger number samples, therefore longer simulations. many iterations need exactly? effective sample size (n.eff) measures useful length chain accounting autocorrelation. recommended check n.eff parameter interest, well relevant combination parameters. general, consider need least \\(\\text{n.eff} \\geq 400\\) independent observations obtain reliable Monte Carlo estimates model parameters. animal survival example, n.eff can computed using effectiveSize() function coda package:expected, n.eff smaller total number MCMC iterations (3000) autocorrelation. proposal standard deviation equals 1 mixing good (n.eff \\(\\geq 400\\)), yielding satisfactory effective sample size.","code":"\n# Generate chains for three proposal standard deviations\nd <- tibble(away = c(0.1, 1, 10)) %>%\n     mutate(accepted_traj = map(away,\n                               metropolis,\n                               steps = n_steps,\n                               inits = 0.1)) %>%\n     unnest(accepted_traj) %>%\n     mutate(proposal_sd = str_c(\"SD = \", away),\n            iter = rep(1:n_steps, times = 3))\n\n# Compute effective sample size\nneff1 <- coda::effectiveSize(d$accepted_traj[d$proposal_sd==\"SD = 0.1\"][-c(1:300)])\nneff2 <- coda::effectiveSize(d$accepted_traj[d$proposal_sd==\"SD = 1\"][-c(1:300)])\nneff3 <- coda::effectiveSize(d$accepted_traj[d$proposal_sd==\"SD = 10\"][-c(1:300)])\ntibble(\"SD\" = c(0.1, 1, 10),\n       \"n.eff\" = round(c(neff1, neff2, neff3)))\n#> # A tibble: 3 × 2\n#>      SD n.eff\n#>   <dbl> <dbl>\n#> 1   0.1    81\n#> 2   1     524\n#> 3  10      77"},{"path":"mcmc.html","id":"what-if-you-have-convergence-problems","chapter":"2 MCMC methods","heading":"2.4.3 What if you have convergence problems?","text":"diagnosing convergence MCMC chain, () often encounter difficulties. section offers practical tips hope useful.mixing poor effective sample size low, may enough increase burn-period /increase number simulations. Using informative priors can also facilitate convergence Markov chains helping MCMC algorithm explore parameter space efficiently (Chapter 4). spirit, choosing better initial values start chain can also help. useful strategy use estimates simpler model MCMC chains already converge.convergence problems persist, often issue model . bug code? typo? error equations? often case programming, best way identify problem reduce model’s complexity start simpler model find wrong.Another piece advice think model first foremost data generator. Simulate data model using realistic parameter values, try recover parameters fitting model simulated data. approach help better understand model works, , much data needed obtain reliable parameter estimates. return technique Chapters 5 6.","code":""},{"path":"mcmc.html","id":"in-summary-1","chapter":"2 MCMC methods","heading":"2.5 In summary","text":"idea Markov chain Monte Carlo (MCMC) methods simulate values Markov chain whose stationary distribution precisely posterior distribution parameters want estimate.idea Markov chain Monte Carlo (MCMC) methods simulate values Markov chain whose stationary distribution precisely posterior distribution parameters want estimate.practice, run several Markov chains starting dispersed initial values.practice, run several Markov chains starting dispersed initial values.discard first iterations (warm-burn-phase) consider convergence reached chains converge regime.discard first iterations (warm-burn-phase) consider convergence reached chains converge regime.point , run chains long enough compute Monte Carlo estimates numerical summaries (example, posterior means credible intervals) parameters.point , run chains long enough compute Monte Carlo estimates numerical summaries (example, posterior means credible intervals) parameters.course, want build implement MCMC methods hand every new analysis, Chapter 3 see make easier.course, want build implement MCMC methods hand every new analysis, Chapter 3 see make easier.","code":""},{"path":"software.html","id":"software","chapter":"3 Practical implementation","heading":"3 Practical implementation","text":"","code":""},{"path":"software.html","id":"introduction-3","chapter":"3 Practical implementation","heading":"3.1 Introduction","text":"chapter, explore two practical tools performing Bayesian statistics minimal effort: NIMBLE brms. NIMBLE brms two R packages implement MCMC algorithms . practice, need specify likelihood priors Bayes’ theorem applied automatically. Thanks syntax close R, packages make step relatively straightforward, even complex models.","code":""},{"path":"software.html","id":"nimble","chapter":"3 Practical implementation","heading":"3.2 NIMBLE","text":"NIMBLE stands Numerical Inference statistical Models using Bayesian Likelihood Estimation. originality NIMBLE separates model-building step model-fitting step, allows great flexibility modeling. package developed team scientists continuously improve capabilities based community feedback. NIMBLE community active https://groups.google.com/g/nimble-users, forum developers respond questions quickly helpfully.use NIMBLE, can follow steps:Build model (likelihood priors).Read data.Specify parameters want make inferences.Provide initial values parameters (per chain).Define MCMC settings: number chains, burn-, number post-burn-iterations.Assess convergence.Interpret results.first, don’t forget load package (install NIMBLE, see https://r-nimble.org/download):Let’s return running example coypu survival. First step: define binomial likelihood uniform prior survival probability \\(\\theta\\) using nimbleCode() function:can check model object indeed contains code:code, y n known, \\(\\theta\\) needs estimated. line y ~ dbinom(theta, n) indicates number survivors follows binomial distribution. prior beta distribution parameters 1 1 (dbeta()), .e. uniform distribution 0 1 (dunif()). Standard distributions available NIMBLE (dnorm, dpois, dmultinom, etc.). Note order lines matter: NIMBLE uses declarative language (specify , ).second step, enter data list:NIMBLE distinguishes data (known values left ~) constants (e.g. loop indices). Declaring values constants can improve computational efficiency, although always intuitive. Fortunately, NIMBLE largely handles automatically may suggest moving objects constants improves performance. ignore distinction , use later Chapter 6.third step tell NIMBLE parameters want monitor. , interested survival probability \\(\\theta\\):general, model contains many quantities, informative need monitored. full control tracked therefore useful.fourth step consists specifying initial values model parameters. minimum, must provide initial values quantities appear left side ~ code supplied data.ensure MCMC algorithm properly explores posterior distribution, run multiple chains different initial values. can specify initial values chain (three chains) list, placed inside another list:Alternatively, can write R function generates random initial values:prefer using functions code compact automatically adapts number chains. use function generate initial values, always good practice set random seed beforehand can reproduce results:Fifth final step: need tell NIMBLE number chains (n.chains), burn-length (n.burnin), total number iterations (n.iter):NIMBLE, specify total number iterations, number posterior samples per chain equal n.iter - n.burnin.side note, determine length warm-period (burn-), can run NIMBLE n.burnin <- 0 hundred thousand iterations inspect parameter trace decide many iterations needed reach convergence.NIMBLE also allows discard samples burn-phase, called thinning. default, thinning = 1 (samples removed), meaning simulations used summarize posterior distributions.now ingredients run model, .e. generate samples posterior distribution parameters via MCMC simulations. use nimbleMCMC() function :NIMBLE performs several internal steps detail . nimbleMCMC() function accepts useful arguments. example, setSeed lets fix random seed inside MCMC call, ensuring obtain exactly chains run—useful reproducibility debugging. can also request summary output summary = TRUE, retrieve MCMC samples coda::mcmc() format samplesAsCodaMCMC = TRUE. Finally, can remove progress bar progressBar = FALSE find depressing long simulations. See ?nimbleMCMC details.Let’s take look results, starting examining mcmc.output object contains:R object mcmc.output list three elements, one MCMC chain. Let’s look, example, first chain:element list matrix. rows correspond 1700 samples posterior distribution \\(\\theta\\) (corresponds n.iter - n.burnin iterations). columns represent parameters monitor, theta., can compute posterior mean \\(\\theta\\):95% credible interval:Let us now visualize posterior distribution \\(\\theta\\) histogram:\nFigure 3.1: Histogram posterior distribution survival probability (\\(\\theta\\)).\nconvenient ways perform Bayesian inferences. use R package MCMCvis summarize visualize MCMC output, can also use ggmcmc, bayesplot, basicMCMCplots.Let’s load MCMCvis:obtain common numerical summaries, use MCMCsummary():can also draw caterpillar plot MCMCplot() visualize posterior distributions:\nFigure 3.2: Caterpillar plot posterior distribution survival probability (\\(\\theta\\)).\npoint represents posterior median, thick bar 50% credible interval, thin bar 95% credible interval.can plot MCMC chain (trace plot) associated posterior density MCMCtrace():\nFigure 3.3: Trace plot posterior density survival probability (\\(\\theta\\)).\nplots used assess chain convergence detect potential estimation issues (see Chapter 2). can also add diagnostics discussed earlier:\nFigure 3.4: Trace plot posterior density survival probability (\\(\\theta\\)) convergence diagnostics.\nmajor advantage MCMC methods provide posterior distribution function parameters applying function draws posterior distributions parameters. example, suppose want compute life expectancy coypus, given \\(\\lambda = -1/\\log(\\theta)\\).example, simply combine theta samples three chains:compute corresponding life expectancy:thus obtain 5100 simulated values posterior distribution \\(\\lambda\\), whose first values :can extract usual summaries:Life expectancy approximately one year. can also visualize posterior distribution life expectancy:\nFigure 3.5: Histogram posterior distribution life expectancy.\nalso compute life expectancy inserting directly NIMBLE model line lambda <- -1/log(theta) adding lambda monitored outputs. approach presented particularly useful large models /large datasets, reduces memory usage.Now can get started. convenience, steps summarized . workflow provided nimbleMCMC() allows build models perform Bayesian inference:section, introduced bare minimum get started NIMBLE. NIMBLE much simple MCMC engine: programming environment gives full control model construction parameter estimation. can write functions distributions, choose MCMC methods , even code algorithms. See manual https://r-nimble.org/html_manual/cha-welcome-nimble.html details.","code":"\nlibrary(nimble)\nmodel <- nimbleCode({\n  # likelihood\n  y ~ dbinom(theta, n)\n  # prior\n  theta ~ dbeta(1, 1) # or dunif(0,1)\n})\nmodel\n#> {\n#>     y ~ dbinom(theta, n)\n#>     theta ~ dbeta(1, 1)\n#> }\ndat <- list(n = 57, y = 19)\npar <- c(\"theta\")\ninit1 <- list(theta = 0.1)\ninit2 <- list(theta = 0.5)\ninit3 <- list(theta = 0.9)\ninits <- list(init1, init2, init3)\ninits\n#> [[1]]\n#> [[1]]$theta\n#> [1] 0.1\n#> \n#> \n#> [[2]]\n#> [[2]]$theta\n#> [1] 0.5\n#> \n#> \n#> [[3]]\n#> [[3]]$theta\n#> [1] 0.9\ninits <- function() list(theta = runif(1,0,1))\ninits()\n#> $theta\n#> [1] 0.3109711\nseed <- 666\nset.seed(seed)\nn.iter <- 2000\nn.burnin <- 300\nn.chains <- 3\nmcmc.output <- nimbleMCMC(code = model, # model\n                          data = dat, # data\n                          inits = inits, # initial values\n                          monitors = par, # parameters to monitor\n                          niter = n.iter, # total number of iterations\n                          nburnin = n.burnin, # burn-in iterations\n                          nchains = n.chains) # number of chains\n#> |-------------|-------------|-------------|-------------|\n#> |-------------------------------------------------------|\n#> |-------------|-------------|-------------|-------------|\n#> |-------------------------------------------------------|\n#> |-------------|-------------|-------------|-------------|\n#> |-------------------------------------------------------|\nstr(mcmc.output)\n#> List of 3\n#>  $ chain1: num [1:1700, 1] 0.407 0.201 0.451 0.273 0.254 ...\n#>   ..- attr(*, \"dimnames\")=List of 2\n#>   .. ..$ : NULL\n#>   .. ..$ : chr \"theta\"\n#>  $ chain2: num [1:1700, 1] 0.507 0.382 0.256 0.365 0.177 ...\n#>   ..- attr(*, \"dimnames\")=List of 2\n#>   .. ..$ : NULL\n#>   .. ..$ : chr \"theta\"\n#>  $ chain3: num [1:1700, 1] 0.317 0.244 0.317 0.362 0.357 ...\n#>   ..- attr(*, \"dimnames\")=List of 2\n#>   .. ..$ : NULL\n#>   .. ..$ : chr \"theta\"\ndim(mcmc.output$chain1)\n#> [1] 1700    1\nhead(mcmc.output$chain1)\n#>          theta\n#> [1,] 0.4070527\n#> [2,] 0.2005720\n#> [3,] 0.4513129\n#> [4,] 0.2725412\n#> [5,] 0.2539956\n#> [6,] 0.4019970\nmean(mcmc.output$chain1[,\"theta\"])\n#> [1] 0.3391349\nquantile(mcmc.output$chain1[,\"theta\"], probs = c(2.5, 97.5)/100)\n#>      2.5%     97.5% \n#> 0.2308179 0.4541410\nmcmc.output$chain1[,\"theta\"] %>%\n  as_tibble() %>%\n  ggplot() +\n  geom_histogram(aes(x = value), color = \"white\") +\n  labs(x = \"Survival probability\")\nlibrary(MCMCvis)\nMCMCsummary(object = mcmc.output, round = 2)\n#>       mean   sd 2.5%  50% 97.5% Rhat n.eff\n#> theta 0.34 0.06 0.22 0.34  0.46    1  4831\nMCMCplot(object = mcmc.output, params = 'theta')\nMCMCtrace(object = mcmc.output,\n          pdf = FALSE,\n          ind = TRUE,\n          params = \"theta\")\nMCMCtrace(object = mcmc.output,\n          pdf = FALSE,\n          ind = TRUE,\n          Rhat = TRUE,\n          n.eff = TRUE,\n          params = \"theta\")\ntheta_samples <- c(mcmc.output$chain1[,\"theta\"],\n                   mcmc.output$chain2[,\"theta\"],\n                   mcmc.output$chain3[,\"theta\"])\nlambda <- -1/log(theta_samples)\nhead(lambda)\n#> [1] 1.1125791 0.6224394 1.2569220 0.7692513 0.7296935 1.0973206\nmean(lambda)\n#> [1] 0.9372371\nquantile(lambda, probs = c(2.5, 97.5)/100)\n#>      2.5%     97.5% \n#> 0.6691676 1.2999116\nlambda %>%\n  as_tibble() %>%\n  ggplot() +\n  geom_histogram(aes(x = value), color = \"white\") +\n  labs(x = \"Life expectancy\")\nmodel <- nimbleCode({\n  y ~ dbinom(theta, n)\n  theta ~ dbeta(1, 1)\n  lambda <- -1/log(theta)\n})\ndat <- list(n = 57, y = 19)\npar <- c(\"theta\", \"lambda\")\ninits <- function() list(theta = runif(1,0,1))\nn.iter <- 5000\nn.burnin <- 1000\nn.chains <- 3\nmcmc.output <- nimbleMCMC(code = model,\n                          data = dat,\n                          inits = inits,\n                          monitors = par,\n                          niter = n.iter,\n                          nburnin = n.burnin,\n                          nchains = n.chains)\nMCMCsummary(object = mcmc.output, round = 2)\nMCMCplot(object = mcmc.output)\nMCMCtrace(object = mcmc.output, pdf = FALSE, ind = TRUE)"},{"path":"software.html","id":"brms","chapter":"3 Practical implementation","heading":"3.3 brms","text":"brms stands Bayesian Regression Models using Stan. package makes possible formulate estimate regression models (see next section Chapters 5 6) intuitive way thanks syntax close lme4 package (R reference mixed models), relying Stan, reference software Bayesian statistics. package constant development; see https://paul-buerkner.github.io/brms/. can get help via https://discourse.mc-stan.org/.use brms, start preparing data:Without forgetting load brms:likelihood binomial running example. brms, can express simply:syntax relatively simple requires explanations. argument y | trials(n) ~ 1 makes possible specify model \\(y\\) successes among \\(n\\) trials, estimate intercept, 1 ~. intercept ? directly survival \\(\\theta\\)? use family = binomial(\"logit\") next line specify brms response variable follows binomial distribution. words, generalized linear model (see Chapter 6) \\(\\text{logit}(\\theta) = \\beta\\) estimate \\(\\beta\\), intercept. arguments iter = 2000, warmup = 300, chains = 3 tell brms use 300 iterations adaptation (burn-), following 1700 inference, 3 chains.Let’s take look results:command displays summary table posterior estimates parameter model. find :Estimate posterior mean.Est.Error standard deviation posterior distribution.l-95% CI u-95% CI bounds 95% credible interval.convergence diagnostic Rhat.Bulk_ESS effective sample size (Tail_ESS another measure effective sample size use ).posterior mean -0.7 far proportion coypus survived winter (\\(19/57 \\approx 0.33\\)). always R implementation generalized linear models (see Chapter 6), parameter estimates given scale link function. , estimated intercept expressed logit scale. convert survival probability (0 1), first extract values generated posterior distribution intercept \\(\\beta\\) function brms::as_draws_matrix():apply inverse logistic function plogis() values obtain whole bunch simulated values posterior distribution survival \\(\\theta\\):thus obtain direct estimate posterior mean survival probability, along 95% credible interval:directly function posterior::summarise_draws():visualize posterior distribution survival probability, just need use (Figure 3.6):\nFigure 3.6: Histogram posterior distribution survival probability (\\(\\theta\\)).\nbrms, can assess convergence MCMC chains (Figure 3.7):\nFigure 3.7: Histogram posterior distribution trace plot survival probability logit scale (b). histogram, x-axis represents possible values intercept (logit scale) y-axis frequency simulated values. trace plot, x-axis corresponds MCMC iteration number y-axis simulated values intercept (logit scale).\ngraph displays trace plots (right) well posterior densities (left).side note, determine length warm-period (burn-), enough run brms warmup = 0 hundred thousand iterations inspect parameter trace decide number iterations needed reach convergence.major advantage MCMC methods allow obtaining posterior distribution function parameters applying function values drawn posterior distributions parameters. Note estimate intercept \\(\\beta\\) therefore already used idea obtain posterior distribution survival probability applying inverse logit function. another example, suppose like compute life expectancy coypus, given \\(\\lambda = -1/\\log(\\theta)\\):Life expectancy approximately one year. can also visualize posterior distribution life expectancy (Figure 3.8):\nFigure 3.8: Histogram posterior distribution life expectancy. x-axis represents different possible values life expectancy. vertical axis indicates number simulated draws (Count) value.\nwhole bunch parameters set default brms; important aware . concerns priors particular. brms, default priors often non-informative weakly informative, always good examine explicitly. following command displays summary priors used already fitted model:brms package uses weakly informative prior Student distribution 3 degrees freedom, centered 0, standard deviation 2.5. 3 degrees freedom give distribution heavier tails normal, provides robustness extreme values. center 0 reflects absence strong prior value intercept. width 2.5 allows reasonably wide variation intercept without completely non-informative.cases, relevant define prior, example reflect knowledge literature constrain estimation (informative prior). , propose normal prior centered 0 standard deviation 1.5 intercept; come back Chapter 4:can use model specification:can check results close obtained default prior:","code":"\ndat <- data.frame(y = 19, n = 57)\nlibrary(brms)\nbayes.brms <- brm(\n  y | trials(n) ~ 1, # the number of successes is a function of an intercept\n  family = binomial(\"logit\"), # binomial family with logit link function\n  data = dat, # data used\n  chains = 3, # number of MCMC chains\n  iter = 2000, # total number of iterations per chain\n  warmup = 300, # number of burn-in iterations\n  thin = 1 # no thinning (each iteration is kept)\n)\nsummary(bayes.brms)\n#>  Family: binomial \n#>   Links: mu = logit \n#> Formula: y | trials(n) ~ 1 \n#>    Data: dat (Number of observations: 1) \n#>   Draws: 3 chains, each with iter = 2000; warmup = 300; thin = 1;\n#>          total post-warmup draws = 5100\n#> \n#> Regression Coefficients:\n#>           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> Intercept    -0.70      0.28    -1.28    -0.17 1.00     1732     2305\n#> \n#> Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n#> and Tail_ESS are effective sample size measures, and Rhat is the potential\n#> scale reduction factor on split chains (at convergence, Rhat = 1).\ndraws_fit <- as_draws_matrix(bayes.brms)\nbeta <- draws_fit[,'Intercept'] # selects the intercept column\ntheta <- plogis(beta)  # logit -> [0,1] conversion\nmean(theta)\n#> [1] 0.3354256\nquantile(theta, probas = c(2.5,97.5)/100)\n#>        0%       25%       50%       75%      100% \n#> 0.1555931 0.2932298 0.3331265 0.3770575 0.5527164\nsummarise_draws(theta)\n#> # A tibble: 1 × 10\n#>   variable   mean median     sd    mad    q5   q95  rhat ess_bulk ess_tail\n#>   <chr>     <dbl>  <dbl>  <dbl>  <dbl> <dbl> <dbl> <dbl>    <dbl>    <dbl>\n#> 1 Intercept 0.335  0.333 0.0617 0.0619 0.235 0.440  1.00    1732.    2305.\ndraws_fit %>%\n  ggplot(aes(x = theta)) +\n  geom_histogram(color = \"white\", fill = \"steelblue\", bins = 30) +\n  labs(x = \"Survival probability\", y = \"Frequency\")\nplot(bayes.brms)\nbeta <- draws_fit[,'Intercept'] # selects the intercept column\ntheta <- plogis(beta)  # logit -> [0,1] conversion\nlambda <- -1 / log(theta) # transforms survival into life expectancy\nsummarize_draws(lambda) # summary of draws: mean, median, intervals\n#> # A tibble: 1 × 10\n#>   variable   mean median    sd   mad    q5   q95  rhat ess_bulk ess_tail\n#>   <chr>     <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>    <dbl>    <dbl>\n#> 1 Intercept 0.928  0.910 0.161 0.153 0.691  1.22  1.00    1732.    2305.\nlambda %>%\n  as_tibble() %>%\n  ggplot() +\n  geom_histogram(aes(x = Intercept), color = \"white\") +\n  labs(x = \"Life expectancy\")\nprior_summary(bayes.brms)\n#> Intercept ~ student_t(3, 0, 2.5)\nnlprior <- prior(normal(0, 1.5), class = \"Intercept\")\nbayes.brms <- brm(y | trials(n) ~ 1,\n                  family = binomial(\"logit\"),\n                  data = dat,\n                  prior = nlprior, # our own priors\n                  chains = 3,\n                  iter = 2000,\n                  warmup = 300,\n                  thin = 1)\nsummary(bayes.brms)\n#>  Family: binomial \n#>   Links: mu = logit \n#> Formula: y | trials(n) ~ 1 \n#>    Data: dat (Number of observations: 1) \n#>   Draws: 3 chains, each with iter = 2000; warmup = 300; thin = 1;\n#>          total post-warmup draws = 5100\n#> \n#> Regression Coefficients:\n#>           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> Intercept    -0.69      0.27    -1.24    -0.18 1.00     1664     2306\n#> \n#> Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n#> and Tail_ESS are effective sample size measures, and Rhat is the potential\n#> scale reduction factor on split chains (at convergence, Rhat = 1)."},{"path":"software.html","id":"summary","chapter":"3 Practical implementation","heading":"3.4 Summary","text":"NIMBLE makes possible model simple situations complex models, great flexibility.NIMBLE makes possible model simple situations complex models, great flexibility.syntax based R, makes easier get started know language.syntax based R, makes easier get started know language.offers full control model algorithms, assumes comfortable programming.offers full control model algorithms, assumes comfortable programming.Conversely, brms makes possible take advantage MCMC methods without write model (likelihood particular).Conversely, brms makes possible take advantage MCMC methods without write model (likelihood particular).syntax simple close lme4, makes particularly suitable generalized linear models (mixed ; see Chapter 6).syntax simple close lme4, makes particularly suitable generalized linear models (mixed ; see Chapter 6).return, brms relies pre-programmed components (model families, etc.), important pay attention default choices, especially regarding prior distributions.return, brms relies pre-programmed components (model families, etc.), important pay attention default choices, especially regarding prior distributions.chapter thus offers first concrete approach implementing Bayesian models, moving richer models, mixed models.chapter thus offers first concrete approach implementing Bayesian models, moving richer models, mixed models.","code":""},{"path":"prior.html","id":"prior","chapter":"4 Prior distributions","heading":"4 Prior distributions","text":"","code":""},{"path":"prior.html","id":"introduction-4","chapter":"4 Prior distributions","heading":"4.1 Introduction","text":"chapter, explore fundamental aspect Bayesian statistics: role prior distributions, priors. see priors interact data via Bayes’ theorem produce posterior distribution, influence varies depending much information data provide. also learn incorporate relevant external information expert knowledge previous studies, critically assess prior choices using simulations.","code":""},{"path":"prior.html","id":"roleprior","chapter":"4 Prior distributions","heading":"4.2 The role of the prior","text":"Bayesian statistics, prior plays essential role: expresses knowledge, uncertainties, , conversely, lack information parameters model. Choosing priors well therefore key step Bayesian analysis. use prior?incorporate existing knowledge: often information previous studies, meta-analyses, expert opinion. prior makes possible formalize include prior knowledge, rather ignoring acting starting nothing. see example Section 4.4.incorporate existing knowledge: often information previous studies, meta-analyses, expert opinion. prior makes possible formalize include prior knowledge, rather ignoring acting starting nothing. see example Section 4.4.deal lack data: data scarce informative, frequentist methods can fail estimate certain parameters correctly (boundary estimates probability, random-effect variance estimated zero). situations, well-chosen prior can help stabilize inference providing complementary information.deal lack data: data scarce informative, frequentist methods can fail estimate certain parameters correctly (boundary estimates probability, random-effect variance estimated zero). situations, well-chosen prior can help stabilize inference providing complementary information.constrain complex models: mixed models, presence parameters difficult estimate, priors make possible bound solution space plausible values avoid aberrant estimates. example, mixed model (see Chapter 6) estimate variance groups levels effect, absence prior can lead unrealistic values numerical instabilities. weakly informative prior can help situation.constrain complex models: mixed models, presence parameters difficult estimate, priors make possible bound solution space plausible values avoid aberrant estimates. example, mixed model (see Chapter 6) estimate variance groups levels effect, absence prior can lead unrealistic values numerical instabilities. weakly informative prior can help situation.prevent overfitting: models many explanatory variables, priors play regularization role penalizing unimportant effects. example, regression includes many covariates, prior form \\(N(0,1.5^2)\\) prevents model assigning overly strong effects weakly informative variables, thereby reducing risk overfitting.prevent overfitting: models many explanatory variables, priors play regularization role penalizing unimportant effects. example, regression includes many covariates, prior form \\(N(0,1.5^2)\\) prevents model assigning overly strong effects weakly informative variables, thereby reducing risk overfitting.choice prior depends directly context scientific question.non-informative prior aims express lack knowledge: often used one want introduce strong assumptions. practice, translates wide uniform distributions. beware: even seemingly vague prior can informative transformed model scale, see Section 4.5.non-informative prior aims express lack knowledge: often used one want introduce strong assumptions. practice, translates wide uniform distributions. beware: even seemingly vague prior can informative transformed model scale, see Section 4.5.informative prior reflects credible knowledge external dataset analyzed: may come literature synthesis, past experience, expert opinion. advantage reduce uncertainty parameters, especially little data. see example Section 4.4.informative prior reflects credible knowledge external dataset analyzed: may come literature synthesis, past experience, expert opinion. advantage reduce uncertainty parameters, especially little data. see example Section 4.4.weakly informative prior somewhat compromise non-informative informative priors. idea rule values clearly aberrant incompatible know phenomenon studied, still leaving enough freedom model learn data. type prior used notably brms. see example Chapter 6.weakly informative prior somewhat compromise non-informative informative priors. idea rule values clearly aberrant incompatible know phenomenon studied, still leaving enough freedom model learn data. type prior used notably brms. see example Chapter 6.practice, cautious strategy start weakly informative prior, centered normal distribution moderate variance, test informative (vague) alternatives examine impact posterior results. idea sensitivity analysis developed Section 4.3.","code":""},{"path":"prior.html","id":"sensibilite","chapter":"4 Prior distributions","heading":"4.3 Sensitivity to the prior","text":"Let us return running example coypu survival. Let us examine different choices priors influence posterior distribution survival probability. Figure 4.1, three increasingly informative priors (columns), two sample sizes (rows).\nFigure 4.1: Combined effect prior sample size posterior distribution binomial likelihood. Columns: three beta priors Beta(1,1), Beta(5,5) Beta(20,1). Rows: small (n = 6, y = 2) large (n = 57, y = 19) sample (factor 10). red line represents prior, black line posterior distribution.\nlittle data (top row), effect prior visible: posterior distribution survival remains close prior, especially \\(\\text{Beta}(20,1)\\) pulls estimate toward high values. data (bottom row), posterior distribution dominated likelihood: concentrates around observed proportion, except prior \\(\\text{Beta}(20,1)\\) posterior distribution centered 0.5. thus observe fundamental principle Bayesian inference: numerous informative data , less prior influences results.can formalize observations made Figure 4.1. Recall likelihood \\(\\text{Bin}(n,\\theta)\\) \\(y\\) successes, prior \\(\\text{Beta}(,b)\\) distribution, posterior distribution also beta (conjugacy), precisely \\(\\text{Beta}(+y,\\;b+n-y)\\). Now, mean \\(\\text{Beta}(,b)\\) \\(\\displaystyle \\frac{}{+b}\\), therefore mean posterior distribution \\(\\text{Beta}(+y,\\;b+n-y)\\) \\(\\displaystyle \\frac{+y}{+b+n}\\), can rewritten weighted average mean prior distribution \\(\\mu_{prior} = \\displaystyle \\frac{}{+b}\\) observed proportion \\(y/n\\), none maximum likelihood estimator \\(\\hat{\\theta}\\), weight \\(w = \\displaystyle \\frac{n}{+b+n}\\). Note: weight statistical sense term, weighting factor, sense “kilograms coypu”. words, mean posterior distribution \\((1-w)\\mu_{prior} + w \\hat{\\theta}\\). Thus, sample size \\(n\\) large, \\(w\\) tends 1, posterior mean approaches maximum likelihood estimator. Conversely, small sample informative prior (sum \\(+b\\) large; see Figure 1.4), \\(w\\) small, prior pulls estimate. short, data limited, rely prior; rich, let likelihood speak.conclusion, always good idea carry kind sensitivity analysis. comparing results obtained different priors (non-informative, weakly informative, informative), can ensure conclusions depend excessively prior choices. , panic: simply means little information parameter question, must extra cautious think carefully prior used. return later.","code":""},{"path":"prior.html","id":"informativeprior","chapter":"4 Prior distributions","heading":"4.4 How to incorporate prior information?","text":"","code":""},{"path":"prior.html","id":"meta-analysis","chapter":"4 Prior distributions","heading":"4.4.1 Meta-analysis","text":"Let us go back running example estimating survival probability, making slightly complex account common issue studying animal populations: imperfect detection individuals. Indeed, depending behavior field conditions, animal may well alive present, detected time sampling. correct bias, capture–recapture protocols often used, rely individual identification animals, via ring, coat pattern, genetic profile, etc.individual can thus detected (1) (0), code example 101 means: seen first year, missed second, seen third. simplest model, assume constant survival probability \\(\\theta\\) constant detection probability \\(p\\). likelihood history 101 therefore: \\(\\Pr(101)=\\theta\\,(1-p)\\,\\theta\\,p\\). obtain full likelihood, perform calculation individual assume share \\(\\theta\\) \\(p\\), independent.take break coypus, let us look White-throated Dipper (Cinclus cinclus), bird studied 40 years Gilbert Marzolin, mathematics teacher passionate ornithology chance work. capture–recapture data 7 years (1981–1987) 200 birds.start non-informative prior survival probability, say \\(\\text{Beta}(1,1)\\). model . alternative prior, can draw accumulated knowledge similar species. passerines, instance, relationship body mass survival probability: average, heavier birds live longer. allometric relationship quantified McCarthy (2007) via linear regression (see Chapter 5), based survival mass data 27 European passerine species. Using regression passerines specific case dipper, knowing dipper weighs average 59.8 grams, can predict annual survival probability. model thus provides estimate 0.57 standard error 0.075. values allow us define informative prior, form normal distribution centered 0.57 variance \\(0.075^2\\). model B.thus obtain following results dipper:rich dataset (7 years), information contained likelihood dominates; informative prior adds almost information, two models produce similar results.Now imagine limited data. happens first three years, example? redo analysis, results now:time, informative prior makes real difference. width interval reduced nearly 50%, bringing mean estimate back toward realistic value passerine. also note posterior estimate model B 3 years data close obtained 7 years (Figure 4.2).\nFigure 4.2: Comparison posterior estimates dipper survival according type prior study duration. point represents posterior mean, 95% credible interval. grey line indicates survival value meta-analysis passerines (0.57).\nexample shows information literature (allometric mass–survival relationship obtained via meta-analysis) can used build relevant informative prior, capable substantially improving precision estimates, especially data limited. approach offers low-cost alternative lengthening field protocols, provided course (relatively simple) question remains estimation single survival.","code":""},{"path":"prior.html","id":"moment-matching-method","chapter":"4 Prior distributions","heading":"4.4.2 Moment-matching method","text":"dipper example, used normal distribution informative prior parameter happens probability. However, normal distribution can take negative values values greater 1, desirable probability. example, informative prior \\(N(0.57, 0.075^2)\\) average 0 1 small variance, little chance goes wrong. can see simulating values R command summary(rnorm(n = 100, mean = 0.57, sd = 0.075)). Still, satisfying.good news can construct appropriate informative prior probability using ‑called “moment-matching” method. moment-matching method consists choosing parameters prior distribution matching moments (often mean variance) represent prior information (seeing data).prior information available form mean \\(\\mu\\) standard deviation \\(\\sigma\\), can transform moments parameters \\(,b\\) beta distribution. reminder, mean variance beta distribution parameters \\(\\) \\(b\\) \\(\\mu=\\dfrac{}{+b}\\) \\(\\sigma^2=\\dfrac{ab}{(+b)^2(+b+1)}\\). inverting relationships, obtain: \\(=\\displaystyle \\Bigl(\\frac{1-\\mu}{\\sigma^2}-\\frac{1}{\\mu}\\Bigr)\\mu^2\\) \\(b=\\displaystyle \\Bigl(\\frac{1}{\\mu}-1\\Bigr)\\). example, \\(\\mu=0.57\\) \\(\\sigma=0.075\\), can deduce \\(= 24.3\\) \\(b = 18.3\\) lines code:can check beta distribution indeed mean standard deviation given meta-analysis:can therefore adopt prior \\(\\text{Beta}(=24.3,\\,b=18.3)\\) incorporate mean information variability obtained allometric survival–mass relationship.moment-matching method apply probabilities. can also used construct prior real-valued parameter, example effect coypu body mass survival (see Chapter 5). Suppose expert says: “80% sure parameter \\(\\theta\\) lies –0.15 0.25.” sentence defines 80% credible interval: \\(\\Pr(\\theta \\[-0.15,0.25]) = 0.80\\). seek normal prior \\(\\theta \\sim N(\\mu,\\sigma^2)\\) reflects exactly information.can start mean \\(\\mu\\). interval symmetric, can directly deduce mean \\(\\mu\\) prior midpoint interval: \\(\\displaystyle{\\mu = \\frac{-0.15+0.25}{2}}=0.05\\).Now let us move standard deviation \\(\\sigma\\). expert states 80% values \\(\\theta\\) –0.15 0.25. normal distribution, proportion can written \\(\\Pr(\\mu - z\\,\\sigma \\leq \\theta \\leq \\mu + z \\, \\sigma) = 0.80\\). means 80% mass distribution contained interval centered \\(\\mu\\) width \\(2z\\sigma\\). level 80%, value \\(z\\) 1.2816 (obtained via qnorm(0.90), 0.90 upper quantile \\(1−\\alpha/2 = 1-20/2\\) \\((1−\\alpha)\\% = 80\\%\\) thus \\(\\alpha = 0.20\\)). Finally, obtain \\(\\sigma = \\displaystyle \\frac{0.25-(-0.15)}{2 \\times 1.2816} \\approx 0.156\\). calculation R:conclude desired informative prior \\(N(\\mu=0.05,\\sigma=0.156)\\). can check everything went well:Visually, Figure 4.3 shows density normal distribution mean \\(\\mu=0.05\\) standard deviation \\(\\sigma=0.156\\). light-blue interval corresponds central 80% credible interval, , interval [−0.15; 0.25] contains 80% probability mass. grey dotted lines indicate bounds interval, black dashed line marks position mean. see , thanks symmetry normal distribution, interval centered around mean, 10% mass lies side outside interval.\nFigure 4.3: Normal distribution mean 0.05 standard deviation 0.156. shaded interval corresponds 80% credible interval, –0.15 0.25.\n","code":"\n# desired mean and standard deviation for the beta distribution\nmu <- 0.57 # mean probability\nsigma <- 0.075 # standard deviation on that probability\n# inverse formulas to obtain the parameters a and b of a beta distribution\na <- ((1 - mu) / (sigma^2) - 1 / mu) * mu^2\nb <- a * (1 / mu - 1)\n# display a and b rounded\nc(a = round(a, 1), b = round(b, 1))\n#>    a    b \n#> 24.3 18.3\n# generate 10,000 values from a Beta distribution with parameters a = 24.3 and b = 18.3\nech_prior <- rbeta(n = 10000, shape1 = 24.3, shape2 = 18.3)\n# empirical mean of the draws (should be close to 0.57)\nmean(ech_prior)\n#> [1] 0.5685004\n# empirical standard deviation of the draws (should be close to 0.075)\nsd(ech_prior)\n#> [1] 0.07496597\n# lower and upper bounds given by the expert\na <- -0.15\nb <-  0.25\n\n# stated confidence level\nlevel <- 0.80\nalpha <- 1 - level\n\n# z value corresponding to an 80% credible interval\nz <- qnorm(1 - alpha / 2)  # ≈ 1.2816\n\n# mean = center of the interval\nmu <- (a + b) / 2\n\n# standard deviation deduced from the interval width\nsigma <- (b - a) / (2 * z)\n\nmu\n#> [1] 0.05\nsigma\n#> [1] 0.1560608\nmu    <- 0.05\nsigma <- 0.1560608\npnorm(c(-0.15, 0.25), mean = mu, sd = sigma)\n#> [1] 0.09999996 0.90000004\n#> 0.10 0.90    # OK: 10% on the left, 90% on the right → 80% in the center"},{"path":"prior.html","id":"surprise","chapter":"4 Prior distributions","heading":"4.5 Beware of so-called non-informative priors","text":"Bayesian statistics, often use non-informative priors. careful: appearances can misleading, especially working parameters defined transformed scales, logit log generalized linear models (Chapter 6). Let us take common example model probability \\(\\theta\\) logit scale via parameter \\(\\beta\\) \\(\\text{logit}(\\theta) = \\beta\\).practice, can use simulations check priors bring unpleasant surprises transformation; call prior predictive checks. happens even fitting model, :simulate values prior \\(\\beta\\) logit scale;apply inverse logit transformation obtain \\(\\theta\\);inspect induced prior distribution \\(\\theta\\) judge whether seems realistic.first choice take prior normal distribution large variance, example \\(\\beta \\sim N(0, 10^2)\\). Steps 1 2 obtained via:problem transformation inverse logit function, simulated values—thus probability \\(\\theta\\)—close 0 1 see Figure 4.4 (left panel), implicitly favors extreme values. go non-informative prior logit scale informative prior (without meaning ) natural scale probability.Another choice take \\(\\beta \\sim N(0, 1.5^2)\\). first two steps simulation can summarized :induced distribution \\(\\theta\\) uniform, covering mainly range values 0.05 0.95 can see Figure 4.4 (right panel), better reflects lack information \\(\\theta\\). second choice right one; speak weakly informative priors.\nFigure 4.4: Comparison two priors obtained probability \\(\\theta = \\text{logit}^{-1}(\\beta)\\) transformation inverse logit function \\(\\beta \\sim N(0, 10^2)\\) \\(\\beta \\sim N(0, 1.5^2)\\). x-axis represents different possible values probability \\(\\theta\\) obtained transformation inverse logit. y-axis indicates frequency simulated draws value.\nalso invariant priors, , priors whose shape accounts scale parameter. Jeffreys’ prior example: maximizes information brought data, remaining invariant reparameterization. example, probability \\(\\theta\\), Jeffreys’ prior \\(\\text{Beta}(0.5, 0.5)\\). prior less flat uniform \\(\\text{Beta}(1, 1)\\). often used one wants objective approach, without introducing subjective information. practice, however, Jeffreys’ prior difficult compute, prefer simulation-based approach ensure transformed parameters reasonable priors.","code":"\nlogit_prior <- rnorm(n = 1000, mean = 0, sd = 10) # simulation\nprior <- plogis(logit_prior) # transformation\nlogit_prior2 <- rnorm(n = 1000, mean = 0, sd = 1.5)\nprior2 <- plogis(logit_prior2)"},{"path":"prior.html","id":"summary-1","chapter":"4 Prior distributions","heading":"4.6 Summary","text":"richer data , less prior influences posterior estimate.richer data , less prior influences posterior estimate.hesitate take time visualize priors natural scale parameters using simulations.hesitate take time visualize priors natural scale parameters using simulations.Moment-matching methods offer practical way transform encode knowledge parameters distributions can serve priors (beta normal, example).Moment-matching methods offer practical way transform encode knowledge parameters distributions can serve priors (beta normal, example).use type prior?use type prior?","code":""},{"path":"lms.html","id":"lms","chapter":"5 Regression","heading":"5 Regression","text":"","code":""},{"path":"lms.html","id":"introduction-5","chapter":"5 Regression","heading":"5.1 Introduction","text":"chapter presents application Bayesian statistics linear regression. use example allows us go bit running example survival. opportunity discuss use model simulate data. also illustrate model comparison validation. use NIMBLE brms compare frequentist approach.","code":""},{"path":"lms.html","id":"linear-regression","chapter":"5 Regression","heading":"5.2 Linear regression","text":"","code":""},{"path":"lms.html","id":"the-model","chapter":"5 Regression","heading":"5.2.1 The model","text":"change things bit, suggest using NIMBLE brms example different survival estimation. Let us focus linear regression.Let us start laying foundations linear model. \\(n\\) measurements response variable \\(y_i\\) \\(\\) ranging 1 \\(n\\). Think example mass (kilograms) coypus running example. associate measurement explanatory variable \\(x_i\\), example average outdoor temperature winter (degrees Celsius) coypus. want study effect temperature mass. simplest assumption linear relationship two, use linear regression model. model includes intercept \\(\\beta_0\\), slope \\(\\beta_1\\) describes effect \\(x_i\\) \\(y_i\\), temperature coypu mass. also need parameter describe residual variability represented variance parameter \\(\\sigma^2\\), captures part variation \\(y_i\\) explained \\(x_i\\). probably already encountered model form: \\(y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i\\) errors \\(\\varepsilon_i\\) assumed independent normally distributed mean 0 variance \\(\\sigma^2\\).intercept \\(\\beta_0\\) gives us mass temperature 0 degrees (\\(x_i = 0\\)). parameter \\(\\beta_1\\) tells us change response variable one‑unit increase (1 degree Celsius) explanatory variable (hence term “slope” parameter). general, (strongly) recommended center (subtract mean) scale (divide standard deviation) values explanatory variable numerical interpretational reasons. Numerical first, allows algorithms, whether frequentist Bayesian, get lost corners parameter space. Interpretation next, intercept \\(\\beta_0\\) interpreted value response variable average value explanatory variable.section, rather analyzing “real” data, , parameters \\(\\beta_0\\), \\(\\beta_1\\) \\(\\sigma\\), simulate artificial data, came real underlying process.","code":""},{"path":"lms.html","id":"simulating-data","chapter":"5 Regression","heading":"5.2.2 Simulating data","text":"mean simulating data? Data analysis data simulation two sides model. analysis, use data estimate parameters model. simulation, fix parameters use model generate data. One reason use simulations exercise forces us really understand model; simulate data model, means fully understood works. many good reasons use simulations. Since truth (parameters model) known, can check model correctly coded. can evaluate bias precision parameter estimates, assess effects violating model assumptions, plan data collection protocol, evaluate power statistical test. short, useful technique toolbox!Let us return example. simulate data according linear regression model, start fixing parameters: \\(\\beta_0 = 0.1\\), \\(\\beta_1 = 1\\) \\(\\sigma^2 = 0.5\\) :simulate \\(n = 100\\) values \\(x_i\\) explanatory variable normal distribution mean 0 standard deviation 1, \\(N(0,1)\\) :Finally, simulate values response variable adding normal error epsilon linear relationship beta0 + beta1 * x :\nFigure 5.1: Simulated data (n = 100) according model \\(y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i\\), \\(\\beta_0 = 0.1\\), \\(\\beta_1 = 1\\) \\(\\sigma = 1\\). red line corresponds regression line.\n","code":"\nbeta0 <- 0.1 # true value of the intercept\nbeta1 <- 1 # true value of the coefficient of x\nsigma <- 0.5 # standard deviation of the errors\nset.seed(666) # to make the simulation reproducible\nn <- 100 # number of observations\nx <- rnorm(n = n, mean = 0, sd = 1) # covariate x simulated from a standard normal distribution\nepsilon <- rnorm(n, mean = 0, sd = sigma) # generate normal errors\ny <- beta0 + beta1 * x + epsilon # add errors to the linear relationship\ndata <- data.frame(y = y, x = x)"},{"path":"lms.html","id":"fitting-with-brms","chapter":"5 Regression","heading":"5.2.3 Fitting with brms","text":"section, use brms fit linear regression model data just generated. everything goes well, estimated parameters close values used generate data. go relatively quickly since covered different steps Chapter 3. syntax close use fit model maximum likelihood lm() function R:Let’s take look numerical summaries convergence diagnostics:default, brms used four chains ran 2000 iterations 1000 iterations used burn-, total 4000 iterations posterior inference. output, Intercept, x sigma correspond respectively parameters \\(\\beta_0\\), \\(\\beta_1\\) \\(\\sigma\\) model. \\(\\hat{R}\\) 3 parameters 1, effective sample sizes satisfactory. credible intervals contain true parameter value used simulate data.check mixing good (Figure 5.2):\nFigure 5.2: Histograms posterior distributions (left column) traces (right column) linear regression parameters. histograms, x-axis represents possible values estimated parameter (intercept, slope, standard deviation) y-axis corresponds frequency posterior sample. trace plots, x-axis indicates MCMC iteration number, y-axis represents simulated value parameter iteration.\n","code":"\nlm.brms <- brm(y ~ x, # formula: y as a function of x\n               data = data, # dataset\n               family = gaussian) # normal distribution\nsummary(lm.brms)\n#>  Family: gaussian \n#>   Links: mu = identity; sigma = identity \n#> Formula: y ~ x \n#>    Data: data (Number of observations: 100) \n#>   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n#>          total post-warmup draws = 4000\n#> \n#> Regression Coefficients:\n#>           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> Intercept     0.06      0.06    -0.05     0.17 1.00     4366     3028\n#> x             1.10      0.06     0.99     1.21 1.00     4188     3147\n#> \n#> Further Distributional Parameters:\n#>       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> sigma     0.57      0.04     0.49     0.65 1.00     4090     3050\n#> \n#> Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n#> and Tail_ESS are effective sample size measures, and Rhat is the potential\n#> scale reduction factor on split chains (at convergence, Rhat = 1).\nplot(lm.brms)"},{"path":"lms.html","id":"weakly-informative-priors","chapter":"5 Regression","heading":"5.2.4 Weakly informative priors","text":"Rather using default priors brms, let’s choose priors. use weakly informative priors, specifically normal mean 0 standard deviation 1.5, \\(N(0, 1.5)\\), regression parameters \\(\\beta_0\\) \\(\\beta_1\\). already discussed weakly informative priors Chapter 4. idea close vague non-informative priors, sense try, weakly informative priors, reflect fact really information model parameters. difference non-informative priors can induce aberrant values saw Chapter 4. still case . Take example \\(N(0, 100)\\) parameters linear relationship links mass coypus temperature, simulate whole bunch values priors, form linear relationship:\nFigure 5.3: Simulation regression lines drawn prior distributions. line corresponds draw parameters: intercept slope ~ N(0, 100).\n\nFigure 5.4: Simulation regression lines drawn prior distributions. line corresponds draw parameters: intercept slope ~ N(0, 1.5).\nobtain reasonable values mass coypus, rarely exceeds 10 kilograms. still negative values, smaller ones, MCMC algorithm cope. also numerical advantage using weakly informative priors: help MCMC methods get lost space possible values parameters estimated, allow focus realistic values parameters. , may impression using data construct priors, whereas said prior reflect information available seeing data. opportunity clarify point bit. important thing prior represents information independent data used likelihood.far focused regression parameters, intercept \\(\\beta_0\\) slope \\(\\beta_1\\). standard deviation, \\(\\sigma\\)? parameter just important: reflects much observations deviate average trend described regression line.One option often considered assign uniform distribution, example \\(\\sigma \\sim U(0, B)\\), natural lower bound (0, since \\(\\sigma\\) always positive), upper bound \\(B\\) difficult choose. maximum value one give standard deviation? cases, apparently reasonable value can turn wide. example, model human heights set \\(\\sigma \\sim U(0, 50)\\) (cm), amounts assuming 95% heights spread 100 cm range around mean—unlikely.flexible realistic alternative use exponential distribution \\(\\sigma \\sim \\exp(\\lambda)\\) \\(\\lambda > 0\\) rate parameter. distribution defined positive values, consistent nature \\(\\sigma\\), favors small values standard deviation leaving possibility \\(\\sigma\\) larger data justify .default, one often takes \\(\\lambda = 1\\). \\(\\lambda = 1\\), mean standard deviation distribution equal \\(1\\), induces modest non-restrictive prior (Figure 5.5).\nFigure 5.5: Comparison two prior distributions standard deviation \\(\\sigma\\): uniform distribution \\(\\text{U}(0,5)\\), gives density 0 5, exponential distribution \\(\\text{Exp}(1)\\), favors small values retaining heavier tail.\ncan formalize model follows:\n\\[\\begin{align}\ny_i &\\sim \\text{Normal}(\\mu_i, \\sigma^2) &\\text{[likelihood]}\\\\\n\\mu_i &= \\beta_0 + \\beta_1 \\; x_i &\\text{[linear relationship]}\\\\\n\\beta_0, \\beta_1 &\\sim \\text{Normal}(0, 1.5) &\\text{[prior parameters]} \\\\\n\\sigma &\\sim \\text{Exp}(1) &\\text{[prior parameters]} \\\\\n\\end{align}\\]Let us specify priors:let’s refit brms:check numerical summaries obtained close obtained default priors, close values used simulate data:, two models give almost thing, surprising data informative enough “take ” prior. interest weakly informative priors much seen small example situations: avoid aberrant values, stabilize MCMC computations, remain useful fewer data complex models.","code":"\n\n# number of lines to simulate\nn_lines <- 100\n\n# draws of intercepts and slopes from the priors\nintercepts <- rnorm(n_lines, mean = 0, sd = 100)\nslopes <- rnorm(n_lines, mean = 0, sd = 100)\n\n# create a data frame\nlines_df <- data.frame()\nfor (i in 1:n_lines) {\n  y_vals <- intercepts[i] + slopes[i] * x\n  temp_df <- data.frame(x = x, y = y_vals, line = as.factor(i))\n  lines_df <- rbind(lines_df, temp_df)\n}\n\n# plot with ggplot2\nggplot(lines_df, aes(x = x, y = y, group = line)) +\n  geom_line(alpha = 0.3) +\n  theme_minimal() +\n  labs(x = \"x\", y = \"y\")\nmyprior <- c(\n  prior(normal(0, 1.5), class = b), # prior on the coefficient of x\n  prior(normal(0, 1.5), class = Intercept), # prior on the intercept\n  prior(exponential(1), class = sigma) # prior on the standard deviation of the error\n)\nlm.brms <- brm(y ~ x, \n               data = data, \n               family = gaussian, \n               prior = myprior)\nsummary(lm.brms)\n#>  Family: gaussian \n#>   Links: mu = identity; sigma = identity \n#> Formula: y ~ x \n#>    Data: data (Number of observations: 100) \n#>   Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n#>          total post-warmup draws = 4000\n#> \n#> Regression Coefficients:\n#>           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> Intercept     0.06      0.06    -0.05     0.18 1.00     3562     2765\n#> x             1.10      0.06     0.99     1.21 1.00     3870     2731\n#> \n#> Further Distributional Parameters:\n#>       Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> sigma     0.57      0.04     0.49     0.66 1.00     3540     2633\n#> \n#> Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n#> and Tail_ESS are effective sample size measures, and Rhat is the potential\n#> scale reduction factor on split chains (at convergence, Rhat = 1)."},{"path":"lms.html","id":"fitting-with-nimble","chapter":"5 Regression","heading":"5.2.5 Fitting with NIMBLE","text":"start writing model:code block, start specifying priors three model parameters: normal prior centered 0 standard deviation 1.5 intercept \\(\\beta_0\\) slope \\(\\beta_1\\), well exponential prior standard deviation \\(\\sigma\\) errors. next part (1:n) loop defines likelihood. specify likelihood observation observation, NIMBLE automatically deduces product likelihoods individuals, corresponds likelihood dataset. observation \\(\\), normal distribution centered beta0 + beta1 * x[], standard deviation sigma. recover relationship \\(y_i = \\beta_0 + \\beta_1 x_i + \\varepsilon_i\\) \\(\\varepsilon_i \\sim N(0,\\sigma^2)\\), strictly equivalent \\(y_i \\sim N(\\beta_0 + \\beta_1 x_i,\\sigma^2)\\).next steps put data list, specify initial values, indicate parameters want output:ingredients run NIMBLE:Let’s inspect results:obtain numerical summaries close obtained brms, close true parameter values used simulate data.convergence, can inspect trace plots:Everything looks good. Mixing correct, convergence diagnostics green.","code":"\nmodel <- nimbleCode({\n  # priors\n  beta0 ~ dnorm(0, sd = 1.5) # normal prior on intercept\n  beta1 ~ dnorm(0, sd = 1.5) # normal prior on coefficient\n  sigma ~ dexp(1) # exponential prior on standard deviation\n  # likelihood\n  for(i in 1:n) {\n    y[i] ~ dnorm(beta0 + beta1 * x[i], sd = sigma) # equiv of yi = beta0 + beta1 * xi + epsiloni\n  }\n})\ndat <- list(x = x, y = y, n = n) # data\ninits <- list(list(beta0 = -0.5, beta1 = -0.5, sigma = 0.1), # inits chain 1\n              list(beta0 = 0, beta1 = 0, sigma = 1), # inits chain 2\n              list(beta0 = 0.5, beta1 = 0.5, sigma = 0.5)) # inits chain 3\npar <- c(\"beta0\", \"beta1\", \"sigma\")\nlm.nimble <- nimbleMCMC(\n  code = model,\n  data = dat,\n  inits = inits,\n  monitors = par,\n  niter = 2000,\n  nburnin = 1000,\n  nchains = 3\n)\nMCMCsummary(lm.nimble, round = 2)\n#>       mean   sd  2.5%  50% 97.5% Rhat n.eff\n#> beta0 0.06 0.06 -0.05 0.06  0.17 1.00  3000\n#> beta1 1.10 0.06  0.99 1.10  1.21 1.00  3000\n#> sigma 0.57 0.04  0.49 0.56  0.65 1.01   772\nMCMCtrace(object = lm.nimble,\n          pdf = FALSE,\n          ind = TRUE,\n          Rhat = TRUE,\n          n.eff = TRUE)"},{"path":"lms.html","id":"maximum-likelihood-fitting","chapter":"5 Regression","heading":"5.2.6 Maximum likelihood fitting","text":"Finally, can compare maximum likelihood fitting, obtained simply command lm(y ~ x, data = data). Everything Figure 5.6:\nFigure 5.6: Comparison parameter estimates (intercept slope) across methods (brms, lm, NIMBLE). Points show posterior means brms NIMBLE, maximum likelihood estimate lm. also show 95% credible intervals (brms NIMBLE) 95% confidence interval (lm). dashed black line indicates true value used simulate data.\nposterior means obtained NIMBLE brms close maximum likelihood estimates intercept slope, lesser extent. credible intervals obtained NIMBLE brms confidence interval obtained maximum likelihood include true parameter values used simulate data. Keep mind single simulation; exercise need repeated many times formally assess distance true values parameter estimates (bias).","code":""},{"path":"lms.html","id":"model-evaluation","chapter":"5 Regression","heading":"5.3 Model evaluation","text":"quality model fit data essential assess much confidence can place parameter estimates. Goodness--fit tests well established frequentist statistics, many can also used simple Bayesian models. case, example, residual analysis.case linear regression, model rests several assumptions. assumptions independence, normality, linearity, homoscedasticity (\\(\\sigma\\) vary explanatory variable). general, can evaluate first two context. two, can visualize fit overlaying estimated regression line observed scatter plot. brms package, gives Figure 5.7:\nFigure 5.7: Linear model fit brms. blue line estimated regression, obtained setting intercept slope posterior means, surrounded 95% credible interval.\nNIMBLE, Figure 5.8:\nFigure 5.8: Linear model fit NIMBLE. blue line estimated regression, obtained setting intercept slope posterior means, surrounded 95% credible interval.\nBayesian methods often used complex models linear regression (mixed models; see Chapter 6), standard turnkey goodness--fit tests. situations, commonly use called posterior predictive checks. idea simulate new datasets posterior distribution model parameters, compare observed data. simulated data resemble real data, suggests model fits well. comparison can done visually using Bayesian p-value quantifies discrepancy simulated observed data.brms, just :\nFigure 5.9: Posterior predictive checks produced brms. black curve corresponds observed data; blue curves data simulated model. x-axis shows possible values simulated observed response. y-axis shows estimated density.\npp_check() function generates posterior predictive check plots (Figure 5.9). compares observed data data simulated fitted model. model fits data well, able use generate data resemble observed data. Therefore, simulated curves overlap observations well, indicates model captures structure data correctly. Otherwise, may suggest model misspecification, example inappropriate link distribution family (see Chapter 6).dedicated function NIMBLE, need simulate data model estimated parameters. hand life expectancy, simplest approach include additional line NIMBLE code:line y_sim[] ~ dnorm(beta0 + beta1 * x[], sd = sigma) added simulate fitted model. data initial values change; just need add y_sim list parameters want retrieve output:rerun NIMBLE:merge 3 chains, select columns corresponding y_sim:take 10 draws, brms default, format results:Finally, obtain posterior predictive checks plot Figure 5.10:\nFigure 5.10: Posterior predictive checks produced NIMBLE. black curve corresponds observed data; blue curves data simulated model. x-axis shows possible values simulated observed response. y-axis shows estimated density.\ncan also compute Bayesian p-value, represents proportion datasets simulated model chosen statistic (mean) large larger observed one. value close 0 1 can indicate poor fit model particular statistic, whereas value close 0.5 suggests good fit. Bayesian p-value obtained follows:brms, can also obtain Bayesian p-value:","code":"\n# extract values from posteriors\npost <- as_draws_df(lm.brms)\n\n# create grid of x values\ngrille_x <- tibble(x = seq(min(data$x), max(data$x), length.out = 100))\n\n# for each x, simulate y values\npred <- post %>%\n  select(b_Intercept, b_x) %>%\n  expand_grid(grille_x) %>%\n  mutate(y = b_Intercept + b_x * x) %>%\n  group_by(x) %>%\n  summarise(\n    mean = mean(y),\n    lower = quantile(y, 0.025),\n    upper = quantile(y, 0.975),\n    .groups = \"drop\"\n  )\n\n# extract post means\nintercept <- summary(lm.brms)$fixed[1,1]\nslope <- summary(lm.brms)$fixed[2,1]\n\n# dataviz\nggplot(data, aes(x = x, y = y)) +\n  geom_point(alpha = 0.6) +\n  geom_ribbon(data = pred, aes(x = x, ymin = lower, ymax = upper), fill = \"blue\", alpha = 0.2, inherit.aes = FALSE) +\n  geom_line(data = pred, aes(x = x, y = mean), color = \"blue\", size = 1.2) +\n  labs(x = \"x\", y = \"y\") +\n  coord_cartesian(xlim = range(grille_x$x)) +\n  theme_minimal()\nx <- data$x\ny <- data$y\n\nposterior <- rbind(lm.nimble$chain1, lm.nimble$chain2, lm.nimble$chain3)\nbeta0 <- posterior[,'beta0']\nbeta1 <- posterior[,'beta1']\n\nx_seq <- seq(min(data$x), max(data$x), length.out = 100)\n\npred_matrix <- sapply(x_seq, function(xi) beta0 + beta1 * xi)\n\npred_df <- tibble(\n  x = x_seq,\n  y_mean = colMeans(pred_matrix),\n  y_lower = apply(pred_matrix, 2, quantile, probs = 0.025),\n  y_upper = apply(pred_matrix, 2, quantile, probs = 0.975)\n)\n\ntrue_df <- tibble(x = x_seq, y_true = 0.1 + 1 * x_seq)\n\nggplot() +\n  geom_point(data = data, aes(x = x, y = y), alpha = 0.6) +\n  geom_ribbon(data = pred_df, aes(x = x, ymin = y_lower, ymax = y_upper), fill = \"blue\", alpha = 0.2) +\n  geom_line(data = pred_df, aes(x = x, y = y_mean), color = \"blue\", size = 1.2) +\n # geom_line(data = true_df, aes(x = x, y = y_true), color = \"red\", size = 1.2) +\n  labs(x = \"x\", y = \"y\") +\n  theme_minimal()\npp_check(lm.brms)\nmodel <- nimbleCode({\n  beta0 ~ dnorm(0, sd = 1.5) # normal prior on intercept\n  beta1 ~ dnorm(0, sd = 1.5) # normal prior on coefficient\n  sigma ~ dexp(1) # exponential prior on standard deviation\n  for(i in 1:n) {\n    y[i] ~ dnorm(beta0 + beta1 * x[i], sd = sigma) # model for observed data\n    y_sim[i] ~ dnorm(beta0 + beta1 * x[i], sd = sigma) # model for simulated data\n  }\n})\npar <- c(\"beta0\", \"beta1\", \"sigma\", \"y_sim\")\nlm.nimble <- nimbleMCMC(\n  code = model,\n  data = dat,\n  inits = inits,\n  monitors = par,\n  niter = 2000,\n  nburnin = 1000,\n  nchains = 3\n)\n#> |-------------|-------------|-------------|-------------|\n#> |-------------------------------------------------------|\n#> |-------------|-------------|-------------|-------------|\n#> |-------------------------------------------------------|\n#> |-------------|-------------|-------------|-------------|\n#> |-------------------------------------------------------|\n# merge\ny_sim_mcmc <- rbind(lm.nimble$chain1, lm.nimble$chain2, lm.nimble$chain3)\n# get columns corresponding to simulated y (y_sim[i])\ny_sim_cols <- grep(\"^y_sim\\\\[\", colnames(y_sim_mcmc))\n# extract\ny_sim_matrix <- y_sim_mcmc[, y_sim_cols]\n# set seed for reproducibility\nset.seed(123)\n# select at random 10 values\nsim_indices <- sample(1:nrow(y_sim_matrix), 10)\n# format simulated data\nsimulations_df <- data.frame(\n  y_sim = as.vector(t(y_sim_matrix[sim_indices, ])), # sim values\n  Replicate = rep(1:length(sim_indices), each = n), # id draw\n  Observation = rep(1:n, times = length(sim_indices)) # id obs\n)\nggplot() +\n  geom_density(aes(x = y_sim, group = Replicate), color = \"lightblue\", alpha = 0.2, data = simulations_df) +\n  geom_density(aes(x = y), color = \"black\", alpha = 0.5, size = 1.2, data = data.frame(y = y)) +\n  labs(x = \"\",\n       y = \"\") +\n  theme_minimal(base_size = 14)\n# Observed test stat\nT_obs <- mean(y)\n\n# Simulated test stat\nT_sim <- apply(y_sim_matrix, 1, mean)\n\n# Bayesian p-value: proportion of simulations where T_sim is more extreme than T_obs\nbayes_pval <- mean(T_sim >= T_obs)\nbayes_pval\n#> [1] 0.512\n# extract simulations\ny_rep <- posterior_predict(lm.brms)\n\n# compute test stat on sim data\nT_sim <- rowMeans(y_rep)\n\n# compute test stat on observed data\nT_obs <- mean(lm.brms$data$y)\n\n# compute Bayesian p-value\nbayes_pval <- mean(T_sim >= T_obs)\nbayes_pval\n#> [1] 0.49075"},{"path":"lms.html","id":"model-comparison","chapter":"5 Regression","heading":"5.4 Model comparison","text":"saw Chapter 1, Bayesian statistics makes possible compare several hypotheses , assess plausible hypothesis given data collected.comparing models, essential ask goal analysis : better understand phenomenon (explanatory approach), rather make predictions (predictive approach)?One strategy build single model includes variables deemed relevant, fit , examine , test , improve progressively. approach aims less identifying best model exploring different variants better understand system study.evaluate model’s predictive ability, one can rely data already used fitting (internal prediction) , reliably, new data (external prediction). latter approach, however, requires splitting data training set test set. possible, still possible estimate predictive performance training data using tools WAIC LOO-CV.WAIC (Watanabe–Akaike Information Criterion) LOO-CV (Leave-One-cross-validation) allow models compared estimating ability predict new data. combine fit observed data penalization model complexity. lower WAIC LOO-CV value indicates better model. WAIC based theoretical approximation, whereas LOO-CV relies cross-validation. LOO-CV generally accurate, especially complex models limited sample sizes, also computationally costly. practice, models well specified sample large, WAIC LOO-CV often give similar results given model.Let us return linear regression example. like test hypothesis variable \\(x\\) explain important part variation \\(y\\). amounts comparing models without variable.brms, fit two models using weakly informative priors:function waic() can used extract WAIC; model smallest value preferred. model \\(x\\) indeed correct one (expect since data simulated), see clearly better one without covariate:Phew, indeed case. function loo() can used compute LOO-CV (approximation, fact):R output, elpd_diff gives difference LOO-CV model one largest value. Thus, best model first line elpd_diff equal zero; , model covariate. therefore reach conclusion WAIC.can also obtain WAIC values NIMBLE. , simply add WAIC = TRUE call function nimbleMCMC:reach conclusion brms. Note NIMBLE directly provide loo() function like brms, even though one estimate LOO-CV “hand”.","code":"# Model with covariate\nfit1 <- brm(y ~ x, data = data, family = gaussian(),\n            prior = c(\n              prior(normal(0, 1.5), class = Intercept),\n              prior(normal(0, 1.5), class = b),\n              prior(exponential(1), class = sigma)\n            ))\n\n# Model without covariate\nfit0 <- brm(y ~ 1, data = data, family = gaussian(),\n            prior = c(\n              prior(normal(0, 1.5), class = Intercept),\n              prior(exponential(1), class = sigma))\n# Compute WAIC\nwaic1 <- waic(fit1)\nwaic0 <- waic(fit0)\n\n# Compare\nwaic1$estimates['waic',]\n#>  Estimate        SE \n#> 172.50456  13.13435\nwaic0$estimates['waic',]\n#>  Estimate        SE \n#> 333.97491  17.23233\n# Leave-one-out cross-validation\nloo1 <- loo(fit1)\nloo0 <- loo(fit0)\n\n# Compare\nloo_compare(loo0, loo1)\n#>      elpd_diff se_diff\n#> fit1   0.0       0.0  \n#> fit0 -80.7       9.1\n# Code of model with covariate\nmodel_avec <- nimbleCode({\n  # priors\n  beta0 ~ dnorm(0, sd = 1.5)\n  beta1 ~ dnorm(0, sd = 1.5)\n  sigma ~ dexp(1)\n  # likelihood\n  for(i in 1:n) {\n    y[i] ~ dnorm(beta0 + beta1 * x[i], sd = sigma)\n  }\n})\n\n# Code of model without covariate\nmodel_sans <- nimbleCode({\n  # priors\n  beta0 ~ dnorm(0, sd = 1.5) \n  sigma ~ dexp(1) \n  # likelihood\n  for(i in 1:n) {\n    y[i] ~ dnorm(beta0, sd = sigma) \n  }\n})\n\n# Data, initial values\ndat <- list(x = x, y = y, n = n) # données\ninits_avec <- list(list(beta0 = -0.5, beta1 = -0.5, sigma = 0.1), # inits chain 1\n                   list(beta0 = 0, beta1 = 0, sigma = 1), # inits chain 2\n                   list(beta0 = 0.5, beta1 = 0.5, sigma = 0.5)) # inits chain 3\ninits_sans <- list(list(beta0 = -0.5, sigma = 0.1), # inits chain 1\n                   list(beta0 = 0, sigma = 1), # inits chain 2\n                   list(beta0 = 0.5, sigma = 0.5)) # inits chain 3\n\n# Model with covariate\nlm.avec <- nimbleMCMC(\n  code = model_avec,\n  data = dat,\n  inits = inits_avec,\n  niter = 2000,\n  nburnin = 1000,\n  nchains = 3,\n  WAIC = TRUE)\n#> |-------------|-------------|-------------|-------------|\n#> |-------------------------------------------------------|\n#> |-------------|-------------|-------------|-------------|\n#> |-------------------------------------------------------|\n#> |-------------|-------------|-------------|-------------|\n#> |-------------------------------------------------------|\n\n# Model without covariate\nlm.sans <- nimbleMCMC(\n  code = model_sans,\n  data = dat,\n  inits = inits_sans,\n  niter = 2000,\n  nburnin = 1000,\n  nchains = 3,\n  WAIC = TRUE)\n#> |-------------|-------------|-------------|-------------|\n#> |-------------------------------------------------------|\n#> |-------------|-------------|-------------|-------------|\n#> |-------------------------------------------------------|\n#> |-------------|-------------|-------------|-------------|\n#> |-------------------------------------------------------|\n#>   [Warning] There are 1 individual pWAIC values that are greater than 0.4. This may indicate that the WAIC estimate is unstable (Vehtari et al., 2017), at least in cases without grouping of data nodes or multivariate data nodes.\n\n# Compute WAIC\nlm.avec$WAIC$WAIC\n#> [1] 172.4424\nlm.sans$WAIC$WAIC\n#> [1] 333.3443"},{"path":"lms.html","id":"in-summary-2","chapter":"5 Regression","heading":"5.5 In summary","text":"Linear regression makes possible model relationship continuous response variable one explanatory variables, accounting residual variability.Linear regression makes possible model relationship continuous response variable one explanatory variables, accounting residual variability.Simulating data model excellent way understand works test code.Simulating data model excellent way understand works test code.Weakly informative prior distributions (\\(N(0, 1.5)\\) coefficients \\(\\text{Exp}(1)\\) \\(\\sigma\\)) help constrain realistic values still allowing model freedom learn data.Weakly informative prior distributions (\\(N(0, 1.5)\\) coefficients \\(\\text{Exp}(1)\\) \\(\\sigma\\)) help constrain realistic values still allowing model freedom learn data.Model validation comparison can performed using posterior predictive checks criteria WAIC. tools make possible evaluate model quality respect data arbitrate competing models.Model validation comparison can performed using posterior predictive checks criteria WAIC. tools make possible evaluate model quality respect data arbitrate competing models.","code":""},{"path":"glms.html","id":"glms","chapter":"6 Generalized linear models, and generalized linear mixed models","heading":"6 Generalized linear models, and generalized linear mixed models","text":"","code":""},{"path":"glms.html","id":"introduction-6","chapter":"6 Generalized linear models, and generalized linear mixed models","heading":"6.1 Introduction","text":"chapter presents application Bayesian statistics extensions linear model seen previous chapter: generalized linear models (GLMs) generalized linear mixed models (GLMMs). start GLM allow us revisit running example coypu (ragondin) survival binary data. use GLMM analyze count data. use GLMM analyze count data. use NIMBLE brms compare frequentist approach.","code":""},{"path":"glms.html","id":"generalized-linear-models-glms","chapter":"6 Generalized linear models, and generalized linear mixed models","heading":"6.2 Generalized linear models (GLMs)","text":"Chapter 5, introduced linear regression \\(y_i \\sim N(\\mu_i,\\sigma^2)\\) \\(\\mu_i = \\beta_0 + \\beta_1 x_i\\), model mean \\(\\mu\\) response variable \\(y\\) function explanatory variable \\(x\\). -called linear model well suited continuous response variable. happens response variable discrete? Let us go back coypu example study number animals survive. apply linear regression data, obtain decimal number coypu, bit annoying count , definition, discrete. Moreover, introduce explanatory variable \\(x_i\\) body mass explain variation number coypu survive, may end negative survival probability, one greater one. ? nothing forces linear model consider values positive less one.saw solution Chapter 1. set \\(z_i = 1\\) coypu \\(\\) survived, \\(z_i = 0\\) otherwise, assume survival event like coin flip probability \\(\\theta\\); words, \\(z_i\\) follows Bernoulli distribution parameter \\(\\theta\\). assume individuals independent share distribution, total number coypu survive winter \\(\\displaystyle\\sum_{=1}^n{z_i} = y\\) follows binomial distribution \\(y \\sim \\text{Bin}(n, \\theta)\\), \\(\\theta\\) survival probability.also saw Chapters 2 3 can use logit function force parameter properly estimated 0 1. amounts writing \\(\\text{logit}(\\theta_i) = \\beta_0 + \\beta_1 x_i\\), explained Figure 6.1.\nFigure 6.1: Left: logit function transforms probability p unbounded continuous value logit(p) lives minus infinity plus infinity. Right: inverse logit function transforms linear combination predictors (linear value figure) probability lives 0 1. logit function used logistic regression (GLM binomial distribution) transform probability (0 1) continuous variable defined real line. , inverse logit function allows return probability scale.\nformalize things bit, :\n\\[\\begin{align}\nz_i &\\sim \\text{Bernoulli}(\\theta_i) &\\text{[likelihood]}\\\\\n\\text{logit}(\\theta_i) &= \\beta_0 + \\beta_1 \\; x_i &\\text{[linear relationship]}\\\\\n\\theta_i &= \\text{logit}^{-1}(\\beta_0 + \\beta_1 \\; x_i) = \\dfrac {e^{\\beta_0 + \\beta_1 \\; x_i}} {1+e^{\\beta_0 + \\beta_1 \\; x_i}} &\\text{[transformed relationship]}\\\\\n  \\beta_0, \\beta_1 &\\sim \\text{Normal}(0, 1.5) &\\text{[prior parameters]} \\\\\n\\end{align}\\]illustrate , can go back coypu data add individual body mass data; , recreate raw data, .e. \\(z_i\\):can now fit two models brms, example (get thing NIMBLE): linear regression logistic regression:way, interpreting coefficients logistic regression easy. People often introduce notion odds ratios help, personally speak . always come back graphical representation relationship probability success (survival ) explanatory variables (body mass ), Figure 6.3. observe positive trend , due randomness simulation (since data generated without effect body mass). Another intuitive way deal use rule 4 proposed Andrew Gelman colleagues. trick divide slope logistic regression 4. gives approximate estimate expected change probability one-unit change explanatory variable, point curve steepest. slope estimated 0.23, example, maximum slope logistic curve (around inflection point, changes shape) approximately \\(0.23/4 = 0.06\\). means increase one unit explanatory variable (, coypu’s body mass increases 1 kg) increases survival probability 6% point slope strongest (go survival probability 0.5 0.53), illustrated Figure 6.2:\nFigure 6.2: Illustration Gelman’s rule 4. , approximate effect coypu body mass survival probability (black logistic curve) around inflection point straight line whose slope given estimated coefficient divided 4 (red dashed line).\ndigressing—let’s return problem applying linear regression binary data. can see Figure 6.3, linear regression amounts fitting unbounded straight line binary data, can lead survival probabilities greater 1 (/smaller 0, even case ). Logistic regression, contrast, naturally constrains predictions 0 1 thanks logit transformation, making suitable choice success/failure variables. way, used Bernoulli formulation introduce explanatory variable measured individual scale, necessary, can go back grouped formulation binomial distribution previous chapters.\nFigure 6.3: Comparison linear regression logistic regression fitted binary data. Linear regression (blue) produces predictions greater 1 (problem survival probability), whereas logistic regression (red) guarantees valid probability estimate.\n","code":"\n# Total number of coypus released, and surviving\nn <- 57\ny <- 19\n\n# Create individual data (0 = dead, 1 = alive)\nz <- c(rep(1, y), rep(0, n - y))\n\n# Add a continuous covariate (ex: mass)\nset.seed(123)\nmass <- rnorm(n, mean = 5, sd = 1)  # mass in kg\n\ndf_bern <- data.frame(survival = z, mass = mass)\n# Fit linear regression\nfit_lm <- brm(survival ~ mass, \n              data = df_bern, \n              family = gaussian())\n\n# Fit logistic regression\nfit_logit <- brm(survival ~ mass, \n                 data = df_bern, \n                 family = bernoulli())"},{"path":"glms.html","id":"generalized-linear-mixed-models-glmms","chapter":"6 Generalized linear models, and generalized linear mixed models","heading":"6.3 Generalized linear mixed models (GLMMs)","text":"","code":""},{"path":"glms.html","id":"introduction-7","chapter":"6 Generalized linear models, and generalized linear mixed models","heading":"6.3.1 Introduction","text":"Often, data collected measured structure: hierarchical grouped—instance, relationship coypu survival body mass across different populations different watersheds. cases, relevant model structure data. helps explain variability mean survival explained body mass, thus yields better estimates. achieve , introduce generalized linear mixed models (GLMMs), combine fixed effects GLMs—representing average effect explanatory variable (body mass coypu example)—random effects representing variability among groups hierarchical levels.random effect? effect random represents random selection units larger population—example, sampling sites individuals; repeat experiment, particular sites individuals matter, matters ability generalize interpretation effects. sense, sex coypu, example, considered random effect; repeat experiment, sex variable still two levels, male female. contrast, treating sites within study area coypu fixed effect allows us say things specific sites, without able generalize “population” sites, study area.Along way, see terms hierarchical models, multilevel models, random-effects models used refer GLMM scientific literature. Sometimes exactly thing; sometimes refers GLMMs slightly modified. avoid confusion, remember GLMMs used analyze data come grouped structure.","code":""},{"path":"glms.html","id":"example","chapter":"6 Generalized linear models, and generalized linear mixed models","heading":"6.3.2 Example","text":"illustrate GLMM concretely, imagine situation want estimate coypu abundance Lez watershed, Montpellier, Lez river runs city. lay ten transects across study area. transect, count number coypu present ten regularly spaced points. interested number coypu (counts) responds temperature. measurements clearly hierarchical: take one count 10 points within 10 transects. protocol illustrated Figure 6.4 inspired book colleague Jason Matthiopoulos (Matthiopoulos 2011).\nFigure 6.4: Diagram coypu data sampling protocol 10 points 10 transects. study area black. top panel shows number coypu, bottom panel shows temperature.\nStarting protocol, let us simulate data following script. make bit challenging assuming , among ten transects, sampling issues three , sample two three points:commented code, make easier read. Nevertheless, explanations different steps order. begin loop (tr 1:transects) simulates data ten transects, one one. time, draw transect-specific random effect (ref), slightly shifts intercept relationship temperature number coypu depending transect. Next, generate temperature sequence (t) randomly drawn starting point small slope changes temperature slightly one point next. temperature, compute expected intensity counting process (ans) assuming linear relationship (log scale), generate observed data () drawing values Poisson distribution mean ans. Finally, gather everything table (sim_simple) can analyze . Figure 6.5 illustrates data obtain:\nFigure 6.5: Relationship number coypu temperature transect, multiple count points (10 transects, except transects 4, 5, 8 3, 2, 3 points) per transect.\n","code":"\nset.seed(123) # for reproducibility\ntransects <- 10 # total number of transects\nnb_points <- c(10, 10, 10, 3, 2, 10, 10, 3, 10, 10) # number of points per transect\ndata <- NULL # object that will store the simulated data\nfor (tr in 1:transects){\n  ref <- rnorm(1, 0, .3) # transect random effect (N(0, 0.3^2))\n  # temperature simulated along the transect:\n  # random starting point between 18 and 22 °C, then a slight slope per segment\n  t <- runif(1, 18, 22) + runif(1, -0.2, 0.2) * 1:10\n  # expected intensity (log scale): linear relationship with temperature\n  ans <- exp(ref + 0.2 * t)\n  # Poisson counts of coypu at each point\n  an <- rpois(nb_points[tr], ans)\n  # stack the points from the current transect\n  data <- rbind(data, cbind(rep(tr, nb_points[tr]), t[1:nb_points[tr]], an))\n}\n# put everything into a data.frame\nsim_simple <- data.frame(\n  Transect    = data[, 1],\n  Temperature = data[, 2],\n  Ragondins    = data[, 3]\n)\nhead(sim_simple)\n#>   Transect Temperature Ragondins\n#> 1        1    19.78911        54\n#> 2        1    19.94232        46\n#> 3        1    20.09553        47\n#> 4        1    20.24874        60\n#> 5        1    20.40194        53\n#> 6        1    20.55515        42"},{"path":"glms.html","id":"the-glm-approach","chapter":"6 Generalized linear models, and generalized linear mixed models","heading":"6.3.3 The GLM approach","text":"want analyse data. dealing binary data beginning chapter, count data. model type response, use Poisson distribution log link,\n\\(\\log(\\theta_i) = \\beta_0 + \\beta_1\\, \\text{temp}_i\\), \\(\\text{temp}_i\\) temperature:\\[\\begin{align}\n  y_i &\\sim \\text{Poisson}(\\theta_i) &\\text{[likelihood]}\\\\\n  \\log(\\theta_i) &= \\beta_{0} + \\beta_1 \\; \\text{temp}_{} &\\text{[linear predictor]} \\\\\n  \\theta_i &= \\exp(\\beta_0 + \\beta_1 \\; \\text{temp}_{}) &\\text{[transformed mean]} \\\\\n  \\beta_0, \\beta_1 &\\sim \\text{Normal}(0, 1.5) &\\text{[priors]} \\\\\n\\end{align}\\]distribution relatively easy handle , among things, single parameter \\(\\text{theta}\\) gives rate occurrence event modelled, average expected number coypus equal parameter.Poisson GLM, coefficients \\(\\beta_0\\) \\(\\beta_1\\) interpreted log scale. precisely, one-unit increase temperature multiplies mean number coypus \\(\\exp(\\beta_1)\\). instance, \\(\\beta_1 = 0.3\\), one-degree increase corresponds expected increase \\(35\\%\\) mean number coypus, \\(\\exp(0.3) \\approx 1.35\\). can also visualise relationship coypu counts temperature, Figures 6.8 6.10 .first model, let us ignore grouping / multilevel structure data (: transects). fit single curve point cloud: complete pooling model:results :ignore observations collected transect, incorrectly assume observations independent. risk draw misleading conclusions: might think single relationship differences actually due transect--transect variation, conversely might miss true trend. model check shows Figure 6.6 fit poor:\nFigure 6.6: Model-check complete pooling model. x-axis shows possible values observed simulated response. y-axis shows estimated density. Simulated distributions (blue) compared observed data (black). Poor overlap indicates lack fit.\naccount structure data, can fit another model transect treated fixed effect. words, fit separate curve transect, intercept, common slope:results :told brms treat transect categorical variable (factor), via .factor(Transect) call brm(). default, first factor level (transect 1) used reference. means intercept \\(\\beta_0\\) estimated model corresponds transect 1, coefficients transects represent (log scale) deviations relative transect 1. example, intercept transect 1 estimated 3.9. shift transect 1 transect 2 estimated 0.04. Therefore, intercept transect 2 : 3.94.can repeat transect obtain transect-specific intercepts, exponentiate recover expected mean number coypus (original scale) average temperature (equals 0 temperature standardised):indeed estimate one intercept per transect (10 intercepts), common slope (temperature effect) shared transects. Note Mean_count values Poisson means. continuous (possibly non-integer) even though observed data integer counts. key feature Poisson models: response discrete, model parameterised continuous mean.fit better, shown Figure 6.7:\nFigure 6.7: Model-check pooling model. x-axis shows possible values observed simulated response. y-axis shows estimated density. Simulated distributions (blue) compared observed data (black).\npooling model improves complete pooling model (Figure 6.8), remains unsatisfying. pooling means fitting independent model transect, without sharing information across groups. creates two issues: () generalise conclusions beyond specific transects observed, (ii) potentially waste information assuming transect nothing learn others. strategy becomes especially inefficient groups observations.\nFigure 6.8: Comparison complete pooling (black) pooling (red) models predict coypu counts function temperature, transect. pooling model fits independent curve transect, whereas complete pooling assumes common relationship.\n","code":"\n# do not forget to standardise the temperature covariate\nsim_simple$Temp <- scale(sim_simple$Temperature)\n\n# complete pooling model\nfit_complete <- brm(Ragondins ~ Temp,\n                    data = sim_simple,              # simulated data\n                    family = poisson(\"log\"))         # Poisson distribution, log link\nsummary(fit_complete)\n#>  Family: poisson \n#>   Links: mu = log \n#> Formula: Ragondins ~ Temp \n#>    Data: sim_simple (Number of observations: 78) \n#>   Draws: 2 chains, each with iter = 5000; warmup = 1000; thin = 1;\n#>          total post-warmup draws = 8000\n#> \n#> Regression Coefficients:\n#>           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> Intercept     4.13      0.01     4.11     4.16 1.00     5374     5177\n#> Temp          0.10      0.01     0.07     0.13 1.00     5936     5368\n#> \n#> Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n#> and Tail_ESS are effective sample size measures, and Rhat is the potential\n#> scale reduction factor on split chains (at convergence, Rhat = 1).\n# no pooling model (transect as a fixed effect)\nfit_nopool <- brm(Ragondins ~ Temp + as.factor(Transect),\n                  data = sim_simple,\n                  family = poisson(\"log\"))\nsummary(fit_nopool)\n#>  Family: poisson \n#>   Links: mu = log \n#> Formula: Ragondins ~ Temp + as.factor(Transect) \n#>    Data: sim_simple (Number of observations: 78) \n#>   Draws: 2 chains, each with iter = 5000; warmup = 1000; thin = 1;\n#>          total post-warmup draws = 8000\n#> \n#> Regression Coefficients:\n#>                     Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> Intercept               3.90      0.05     3.81     3.99 1.00     3659     4650\n#> Temp                    0.20      0.06     0.08     0.32 1.00     2684     3611\n#> as.factorTransect2      0.04      0.12    -0.21     0.28 1.00     3001     3973\n#> as.factorTransect3     -0.12      0.07    -0.26     0.03 1.00     4207     5527\n#> as.factorTransect4      0.05      0.11    -0.16     0.26 1.00     3737     4859\n#> as.factorTransect5      0.09      0.10    -0.12     0.29 1.00     5687     5484\n#> as.factorTransect6      0.49      0.10     0.29     0.68 1.00     3009     4224\n#> as.factorTransect7      0.19      0.09     0.02     0.37 1.00     3322     4124\n#> as.factorTransect8      0.08      0.09    -0.09     0.26 1.00     5512     5480\n#> as.factorTransect9      0.28      0.08     0.13     0.43 1.00     3452     4601\n#> as.factorTransect10     0.64      0.06     0.53     0.77 1.00     3729     4715\n#> \n#> Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n#> and Tail_ESS are effective sample size measures, and Rhat is the potential\n#> scale reduction factor on split chains (at convergence, Rhat = 1).\n# extract the intercept (reference = Transect 1)\nbeta0 <- fixef(fit_nopool)[\"Intercept\", \"Estimate\"]\n\n# all model coefficients\ncoefs <- fixef(fit_nopool)\n\n# effects associated with the other transects\ncoefs_transects <- coefs[grep(\"as.factor\", rownames(coefs)), \"Estimate\"]\n\n# compute intercepts by transect (on the log scale)\nintercepts_log <- c(\n  Transect1 = beta0,\n  beta0 + coefs_transects\n)\n\n# expected mean counts on the original scale\nmeans <- exp(intercepts_log)\n\n# summary table\ndf_intercepts <- data.frame(\n  Transect = names(intercepts_log),\n  Intercept_log = round(intercepts_log, 2),\n  Mean_count = round(means, 2)\n)\n\n# display\ndf_intercepts\n#>                                Transect Intercept_log Mean_count\n#> Transect1                     Transect1          3.90      49.60\n#> as.factorTransect2   as.factorTransect2          3.94      51.48\n#> as.factorTransect3   as.factorTransect3          3.79      44.15\n#> as.factorTransect4   as.factorTransect4          3.95      51.94\n#> as.factorTransect5   as.factorTransect5          3.99      54.02\n#> as.factorTransect6   as.factorTransect6          4.39      80.75\n#> as.factorTransect7   as.factorTransect7          4.10      60.22\n#> as.factorTransect8   as.factorTransect8          3.98      53.74\n#> as.factorTransect9   as.factorTransect9          4.19      65.76\n#> as.factorTransect10 as.factorTransect10          4.55      94.53"},{"path":"glms.html","id":"the-glmm-approach","chapter":"6 Generalized linear models, and generalized linear mixed models","heading":"6.3.4 The GLMM approach","text":"Let us return objective: assess effect temperature coypu abundance accounting hierarchical structure data (segments nested within transects). far, complete pooling pooling models represented two extremes: either transects share exactly temperature–abundance relationship, transect completely independent relationship (via transect-specific intercept). Generalised linear mixed models (GLMMs) implement realistic compromise, often called partial pooling.build GLMM transect intercept—.e., baseline abundance—intercepts treated unrelated. Instead, modelled random deviations around global intercept \\(\\beta_0\\), drawn common normal distribution. means transect-specific intercepts \\(\\beta_{0j}\\) (\\(j = 1,\\dots,10\\)) viewed coming larger population possible transects, baseline abundance varies across space. capture heterogeneity random effect, written \\(\\beta_{0j} \\sim \\text{Normal}(\\beta_0, \\sigma)\\), \\(\\sigma\\) -transect variability.Equivalently, transect intercept can written \\(\\beta_{0j} = \\beta_0 + b_j\\), \\(b_j \\sim \\text{Normal}(0, \\sigma)\\). \\(\\beta_0\\) intercept “typical” transect, \\(\\sigma\\) quantifies much transects vary around mean. instance, \\(\\beta_0 = 2\\) transect 4 \\(\\beta_{04} = 3\\), transect-specific deviation \\(b_4 = 1\\), corresponding higher--average abundance.hierarchical structure shares information across groups, particularly useful transects observations. can also view partial pooling model (3 parameters estimated : \\(\\beta_0, \\beta_1, \\sigma\\)) compromise complete pooling model (2 parameters: \\(\\beta_0, \\beta_1\\)) pooling model (11 parameters: 10 intercepts 1 common slope \\(\\beta_1\\)).Formally, GLMM can written :\\[\\begin{align}\n  y_i &\\sim \\text{Poisson}(\\theta_i) &\\text{[likelihood]}\\\\\n  \\log(\\theta_i) &= \\beta_{0j} + \\beta_1 \\; \\text{temp}_{} &\\text{[linear predictor]} \\\\\n  \\beta_{0j} &\\sim \\text{Normal}(\\beta_0, \\sigma) &\\text{[random effect]} \\\\\n  \\beta_0 &\\sim \\text{Normal}(0, 1.5) &\\text{[prior mean intercept]} \\\\\n  \\sigma &\\sim \\text{Exp}(1) &\\text{[prior random-effect SD]} \\\\\n  \\beta_1 &\\sim \\text{Normal}(0, 1.5) &\\text{[prior slope]} \\\\\n\\end{align}\\]","code":""},{"path":"glms.html","id":"fitting-the-model-with-brms","chapter":"6 Generalized linear models, and generalized linear mixed models","heading":"6.3.4.1 Fitting the model with brms","text":"first fit partial pooling GLMM brms:syntax, random intercept specified (1 | Transect), 1 means modelling intercept, | indicates “one intercept per transect”. wanted include random slope well, write (1 + Temp | Transect).results :summary reports posterior estimates fixed effects standard deviations random effects. line sd(Intercept) corresponds \\(\\sigma\\), close value 0.3 used simulate data (credible interval includes true value). lines Intercept Temp provide estimates \\(\\beta_0\\) \\(\\beta_1\\) log scale. see check estimates close values used simulation.can also inspect posterior densities trace plots (Figure 6.9):\nFigure 6.9: Convergence diagnostics partial pooling model. histograms (left column), x-axis shows possible parameter values (intercept, slope, SD) y-axis shows frequency posterior sample. trace plots (right column), x-axis shows MCMC iteration y-axis shows sampled parameter value.\ncan now update Figure 6.8 Figure 6.10:\nFigure 6.10: Comparison among complete pooling (black), pooling (red), partial pooling (blue) models predict coypu counts function temperature, transect. pooling fits separate curve transect, complete pooling assumes common relationship, partial pooling provides compromise via transect random effect.\nsee partial pooling fit similar pooling fit, much better complete pooling. small difference transects sampling points (transects 4, 5, 8): , partial pooling closer complete pooling. absence much information transects, reasonable estimates pulled toward overall mean rather toward extreme, transect-specific values.information-sharing mechanism known borrowing strength. leads shrinkage, buffers tendency pooling (fixed-effect) model overfit. sense, partial pooling also provides form regularisation.Model fit validated Figure 6.11:\nFigure 6.11: Model-check partial pooling model. x-axis shows possible values observed simulated response. y-axis shows estimated density. Simulated distributions (blue) compared observed data (black).\nfitting model standardised predictor (, temperature), coefficients \\(\\beta_0\\) \\(\\beta_1\\) interpreted modified scale: \\(\\beta_1\\) corresponds one-standard-deviation change temperature, \\(\\beta_0\\) corresponds expected value standardised temperature equals 0 (.e., mean temperature). practice often want effects natural units (degrees Celsius) rather standard deviations. can convert coefficients back original scale using:\\[\n\\beta_1^{\\text{original}} = \\frac{\\beta_1^{\\text{standardised}}}{\\text{SD}(\\text{temperature})}\n\\]\\[\n\\beta_0^{\\text{original}} = \\beta_0^{\\text{standardised}} - \\beta_1^{\\text{standardised}} \\times \\frac{\\text{Mean}(\\text{temperature})}{\\text{SD}(\\text{temperature})}\n\\]R, can use:can visualise coefficients original scale compare values used simulation (Figures 6.12 6.13):\nFigure 6.12: Posterior distribution mean intercept (original scale). red line indicates true value (0).\n\nFigure 6.13: Posterior distribution temperature effect (original scale). red line indicates true value (0.2).\nrecover parameters used simulate data (red). one simulation run, normal posterior mode coincide exactly true value. Repeating simulation many times allow us assess bias variability formally.bonus, let us compare models without temperature effect. evaluates whether temperature useful predictor:compute WAIC model compare:conclusion, model including temperature provides better fit according WAIC—reassuring since data simulated model.","code":"\n# partial pooling model (transect random intercept)\nfit_partial <- brm(Ragondins ~ Temp + (1 | Transect), # count ~ temperature with a transect random intercept\n                   data = sim_simple,\n                   family = poisson(\"log\"))\nsummary(fit_partial)\n#>  Family: poisson \n#>   Links: mu = log \n#> Formula: Ragondins ~ Temp + (1 | Transect) \n#>    Data: sim_simple (Number of observations: 78) \n#>   Draws: 2 chains, each with iter = 5000; warmup = 1000; thin = 1;\n#>          total post-warmup draws = 8000\n#> \n#> Multilevel Hyperparameters:\n#> ~Transect (Number of levels: 10) \n#>               Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> sd(Intercept)     0.27      0.08     0.16     0.47 1.00     2119     3183\n#> \n#> Regression Coefficients:\n#>           Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\n#> Intercept     4.09      0.09     3.92     4.27 1.00     2087     2747\n#> Temp          0.17      0.05     0.06     0.27 1.00     3559     4119\n#> \n#> Draws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\n#> and Tail_ESS are effective sample size measures, and Rhat is the potential\n#> scale reduction factor on split chains (at convergence, Rhat = 1).\nplot(fit_partial)\n# extract posterior draws for fixed effects\npost <- as_draws_matrix(fit_partial)\nsbzero <- post[, \"b_Intercept\"]\nsbun   <- post[, \"b_Temp\"]\n\n# mean and SD of temperature\nmu <- attr(scale(sim_simple$Temperature), \"scaled:center\")\nsg <- attr(scale(sim_simple$Temperature), \"scaled:scale\")\n\n# convert standardised coefficients to the original scale\nbun   <- sbun / sg                 # beta1 (original scale)\nbzero <- sbzero - sbun * mu / sg   # beta0 (original scale)\ntibble(b0 = bzero) %>%\n  ggplot(aes(x = b0)) +\n  geom_histogram(color = \"white\", fill = \"skyblue\", bins = 30) +\n  geom_vline(xintercept = 0, color = \"red\", linewidth = 1.2) +\n  labs(\n    x = expression(beta[0]),\n    y = \"Frequency\"\n  ) +\n  theme_minimal()\ntibble(b1 = bun) %>%\n  ggplot(aes(x = b1)) +\n  geom_histogram(color = \"white\", fill = \"skyblue\", bins = 30) +\n  geom_vline(xintercept = 0.2, color = \"red\", linewidth = 1.2) +\n  labs(\n    x = expression(beta[1]),\n    y = \"Frequency\"\n  ) +\n  theme_minimal()\n# partial pooling model without temperature\nfit_partial2 <- brm(Ragondins ~ 1 + (1 | Transect),\n                    data = sim_simple,\n                    family = poisson(\"log\"))\nwaic1 <- waic(fit_partial)\nwaic2 <- waic(fit_partial2)\n\ntibble(\n  Model = c(\"With temperature\", \"Without temperature\"),\n  WAIC  = c(waic1$estimates[\"waic\", \"Estimate\"],\n            waic2$estimates[\"waic\", \"Estimate\"])\n)\n#> # A tibble: 2 × 2\n#>   Model                WAIC\n#>   <chr>               <dbl>\n#> 1 With temperature     542.\n#> 2 Without temperature  551."},{"path":"glms.html","id":"fitting-the-model-with-nimble","chapter":"6 Generalized linear models, and generalized linear mixed models","heading":"6.3.4.2 Fitting the model with NIMBLE","text":"now repeat analysis GLMM NIMBLE. start writing model code:Let us comment briefly code. loop (1:n), define Poisson likelihood observation, count[] ~ dpois(theta[]). intensity theta[] (expected number coypus) depends two components: intercept[transect[]] intercept specific transect observation \\(\\) belongs, beta1 * x[] linear temperature effect.term intercept[transect[]] illustrates nested indexing: observation \\(\\), retrieve appropriate intercept vector transect-specific intercepts (intercept[j]) using index transect[]. vector transect contains, observation \\(\\), identifier transect belongs . example, observation 5 belongs transect 3, transect[5] = 3 use intercept[3]. avoids writing double loop transects: observation dynamically picks intercept corresponds group.block (j 1:nbtransects){ intercept[j] ~ dnorm(beta0, sd = sigma) } defines hierarchical structure: transect intercepts estimated independently (fixed-effect model), treated random draws around global mean beta0, -transect variability sigma.read constants data:specify initial values two MCMC chains:also specify parameters monitor MCMC settings:Finally, run NIMBLE:results :brms, coefficients estimated standardised temperature scale. return original scale use:\\[\n\\beta_1^{\\text{original}} = \\frac{\\beta_1^{\\text{standardised}}}{\\text{SD}(\\text{temperature})}\n\\]\\[\n\\beta_0^{\\text{original}} = \\beta_0^{\\text{standardised}} - \\beta_1^{\\text{standardised}} \\times \\frac{\\text{Mean}(\\text{temperature})}{\\text{SD}(\\text{temperature})}\n\\]R:can visualise coefficients original scale compare simulation values (Figures 6.14 6.15):\nFigure 6.14: Posterior distribution mean intercept (original scale). red line indicates true value (0).\n\nFigure 6.15: Posterior distribution temperature effect (original scale). red line indicates true value (0.2).\nrecover parameters used simulate data (red). brms, ran one simulation, normal match true values exactly. Repeating simulations many times provide formal assessment.Let us compare models without temperature using WAIC. need fit model without temperature:also need rerun full model WAIC = TRUE (shown ).can compare WAIC:","code":"\nmodel <- nimbleCode({\n  for (i in 1:n){\n    count[i] ~ dpois(theta[i])                # Poisson likelihood\n    log(theta[i]) <- intercept[transect[i]] + # random intercept by transect\n                     beta1 * x[i]             # temperature effect\n  }\n\n  for (j in 1:nbtransects){\n    intercept[j] ~ dnorm(beta0, sd = sigma)   # intercepts ~ Normal(beta0, sigma)\n  }\n\n  beta0 ~ dnorm(0, sd = 1.5)                  # prior for the mean intercept\n  sigma ~ dexp(1)                             # prior for the random-effect SD\n  beta1 ~ dnorm(0, sd = 1.5)                  # prior for the slope\n})\nmy.constants <- list(\n  n = nrow(sim_simple),       # number of observations\n  nbtransects = transects     # number of transects\n)\n\nmy.data <- list(\n  x = as.vector(sim_simple$Temp),               # standardised covariate\n  count = sim_simple$Ragondins,                 # counts\n  transect = as.numeric(sim_simple$Transect)    # transect ID\n)\ninit1 <- list(\n  intercept = rnorm(transects),\n  beta1 = rnorm(1),\n  beta0 = rnorm(1),\n  sigma = rexp(1)\n)\n\ninit2 <- list(\n  intercept = rnorm(transects),\n  beta1 = rnorm(1),\n  beta0 = rnorm(1),\n  sigma = rexp(1)\n)\n\ninitial.values <- list(init1, init2)\nparameters.to.save <- c(\"beta1\", \"beta0\", \"sigma\")\nn.iter   <- 5000   # total iterations\nn.burnin <- 1000   # burn-in\nn.chains <- 2      # number of chains\nmcmc.output <- nimbleMCMC(\n  code = model,\n  data = my.data,\n  constants = my.constants,\n  inits = initial.values,\n  monitors = parameters.to.save,\n  niter = n.iter,\n  nburnin = n.burnin,\n  nchains = n.chains,\n  progressBar = FALSE\n)\nMCMCsummary(object = mcmc.output, round = 2)\n#>       mean   sd 2.5%  50% 97.5% Rhat n.eff\n#> beta0 4.07 0.09 3.88 4.07  4.24 1.00  4587\n#> beta1 0.17 0.05 0.07 0.16  0.28 1.03    77\n#> sigma 0.26 0.08 0.16 0.25  0.46 1.00   856\n# concatenate the two chains\nsamples <- rbind(mcmc.output$chain1, mcmc.output$chain2)\n\n# posterior draws for coefficients (standardised scale)\nsbzero <- samples[, \"beta0\"]  # beta0 (standardised)\nsbun   <- samples[, \"beta1\"]  # beta1 (standardised)\n\n# mean and SD of temperature\nmu <- attr(scale(sim_simple$Temperature), \"scaled:center\")\nsg <- attr(scale(sim_simple$Temperature), \"scaled:scale\")\n\n# convert to original scale\nbun   <- sbun / sg                 # beta1 (original scale)\nbzero <- sbzero - sbun * mu / sg   # beta0 (original scale)\ntibble(b0 = bzero) %>%\n  ggplot(aes(x = b0)) +\n  geom_histogram(color = \"white\", fill = \"skyblue\", bins = 30) +\n  geom_vline(xintercept = 0, color = \"red\", linewidth = 1.2) +\n  labs(\n    x = expression(beta[0]),\n    y = \"Frequency\"\n  ) +\n  theme_minimal()\ntibble(b1 = bun) %>%\n  ggplot(aes(x = b1)) +\n  geom_histogram(color = \"white\", fill = \"skyblue\", bins = 30) +\n  geom_vline(xintercept = 0.2, color = \"red\", linewidth = 1.2) +\n  labs(\n    x = expression(beta[1]),\n    y = \"Frequency\"\n  ) +\n  theme_minimal()\n# model code without temperature\nmodel.null <- nimbleCode({\n  for (i in 1:n){\n    count[i] ~ dpois(theta[i])\n    log(theta[i]) <- intercept[transect[i]]\n  }\n  for (j in 1:nbtransects){\n    intercept[j] ~ dnorm(beta0, sd = sigma)\n  }\n  beta0 ~ dnorm(0, sd = 1.5)\n  sigma ~ dexp(1)\n})\n\n# run the null model\nparameters.null <- c(\"beta0\", \"sigma\")\n\nmcmc.null <- nimbleMCMC(\n  code = model.null,\n  data = my.data,\n  constants = my.constants,\n  inits = list(beta0 = 0, sigma = 1, intercept = rnorm(transects)),\n  monitors = parameters.null,\n  niter = n.iter,\n  nburnin = n.burnin,\n  nchains = n.chains,\n  progressBar = FALSE,\n  WAIC = TRUE\n)\n#>   [Warning] There are 3 individual pWAIC values that are greater than 0.4. This may indicate that the WAIC estimate is unstable (Vehtari et al., 2017), at least in cases without grouping of data nodes or multivariate data nodes.\n# WAIC values\nwaic.full <- mcmc.output$WAIC$WAIC\nwaic.null <- mcmc.null$WAIC$WAIC\n\n# comparison table\ntibble(\n  Model = c(\"With temperature\", \"Without temperature\"),\n  WAIC  = c(waic.full, waic.null)\n)\n#> # A tibble: 2 × 2\n#>   Model                WAIC\n#>   <chr>               <dbl>\n#> 1 With temperature     542.\n#> 2 Without temperature  552."},{"path":"glms.html","id":"frequentist-fit-with-lme4","chapter":"6 Generalized linear models, and generalized linear mixed models","heading":"6.3.4.3 Frequentist fit with lme4","text":"close chapter, let us run GLMM analysis frequentist framework using lme4.load package:fit GLMM (note brms syntax inspired lme4):Results:read output?notice parameter estimates close obtained brms NIMBLE.","code":"\nlibrary(lme4)\nfit_lme4 <- glmer(\n  Ragondins ~ Temp + (1 | Transect),  # full formula\n  data   = sim_simple,               # simulated data set\n  family = poisson                   # Poisson family for counts\n)\nsummary(fit_lme4)\n#> Generalized linear mixed model fit by maximum likelihood (Laplace\n#>   Approximation) [glmerMod]\n#>  Family: poisson  ( log )\n#> Formula: Ragondins ~ Temp + (1 | Transect)\n#>    Data: sim_simple\n#> \n#>       AIC       BIC    logLik -2*log(L)  df.resid \n#>     568.3     575.3    -281.1     562.3        75 \n#> \n#> Scaled residuals: \n#>     Min      1Q  Median      3Q     Max \n#> -1.9501 -0.6223 -0.1098  0.4779  2.3897 \n#> \n#> Random effects:\n#>  Groups   Name        Variance Std.Dev.\n#>  Transect (Intercept) 0.04402  0.2098  \n#> Number of obs: 78, groups:  Transect, 10\n#> \n#> Fixed effects:\n#>             Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)  4.08804    0.06898  59.266  < 2e-16 ***\n#> Temp         0.15797    0.04863   3.248  0.00116 ** \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Correlation of Fixed Effects:\n#>      (Intr)\n#> Temp -0.106"},{"path":"glms.html","id":"summary-2","chapter":"6 Generalized linear models, and generalized linear mixed models","heading":"6.4 Summary","text":"Generalised linear models (GLMs) extend linear models situations normal error assumption appropriate.Generalised linear models (GLMs) extend linear models situations normal error assumption appropriate.general idea use distribution adapted response – Bernoulli/binomial binary responses (0/1), Poisson counts (0, 1, 2, …) – link mean distribution predictors link function (logit log).general idea use distribution adapted response – Bernoulli/binomial binary responses (0/1), Poisson counts (0, 1, 2, …) – link mean distribution predictors link function (logit log).Adding random effects allows us model hierarchical grouping structures (e.g., sites, individuals, transects), capturing heterogeneity sharing information across groups.Adding random effects allows us model hierarchical grouping structures (e.g., sites, individuals, transects), capturing heterogeneity sharing information across groups.Generalised linear mixed models (GLMMs) jointly estimate fixed effects (population-level) random effects (group-level, assumed drawn common distribution).Generalised linear mixed models (GLMMs) jointly estimate fixed effects (population-level) random effects (group-level, assumed drawn common distribution).complete pooling model ignores group structure assumes data follow relationship. can bias conclusions groups truly differ. pooling model fits separate relationships per group information sharing, leading highly variable estimates groups small sample sizes. Partial pooling (GLMMs / hierarchical models) compromise: groups parameters, linked common distribution. improves stability still respecting -group differences.complete pooling model ignores group structure assumes data follow relationship. can bias conclusions groups truly differ. pooling model fits separate relationships per group information sharing, leading highly variable estimates groups small sample sizes. Partial pooling (GLMMs / hierarchical models) compromise: groups parameters, linked common distribution. improves stability still respecting -group differences.","code":""},{"path":"conclusions.html","id":"conclusions","chapter":"Conclusions","heading":"Conclusions","text":"","code":""},{"path":"conclusions.html","id":"what-we-covered","chapter":"Conclusions","heading":"What we covered","text":"hope book (least little) demystified Bayesian statistics MCMC methods. also hope given tools understand difference frequentist Bayesian approaches, better read “Methods” section papers using Bayesian statistics, gain certain level autonomy conducting Bayesian analyses.Throughout book, covered several essential steps. began exploring motivations using Bayesian approach. introduced Bayes’ theorem discussed interpretation. discovered Markov chain Monte Carlo (MCMC) methods, worked two powerful tools, NIMBLE brms, fit complex models. Particular attention given role prior distributions, whether non-informative informative, well use approaches case studies involving GLM GLMM.","code":""},{"path":"conclusions.html","id":"bayesian-statistics-in-a-nutshell","chapter":"Conclusions","heading":"Bayesian statistics, in a nutshell","text":"Bayesian approach offers many advantages. allows uncertainty quantified coherently using probability, enables explicit integration prior knowledge, makes possible fit complex models via MCMC. addition, Bayesian credible intervals intuitive frequentist confidence intervals.caution nevertheless required. Checking convergence MCMC chains crucial step, sometimes laborious one. choice prior distributions requires careful consideration. Model fit must always evaluated. Finally, computational cost negligible, especially complex models /large datasets.","code":""},{"path":"conclusions.html","id":"a-few-tips","chapter":"Conclusions","heading":"A few tips","text":"finishing, like leave tips inspired experience. tips necessarily specific Bayesian statistics, worth worth.First, take time clearly formulate question. may seem obvious, step essential stay track make right choices, example deciding use subset data answer specific question.Next, think model first, formalize either equations, drawing , words. nature data, therefore, regression framework, family distributions use saw Chapters 5 (normal) 6 (Bernoulli/binomial Poisson)? rush keyboard. Make sure understand , example explaining colleagues.note, remember run simulations. Simulating data model often helps understand better, Chapters 5 6. excellent way test assumptions diagnose potential issues.Choose R environment comfortable ; illustrated brms NIMBLE (Chapter 2), solutions exist.fitting model, start simple. model parameters constant good baseline. ensures data read formatted correctly, outliers (extra zero, misplaced comma), priors generate unexpected behavior (see Chapter 4). approach particularly important Bayesian statistics ensure good performance convergence MCMC algorithm (Chapter 2), also giving idea time required run analysis. everything looks good, gradually add complexity, random effects example (Chapter 6), reach model structure seems appropriate answer question. likely implies several iterations fitting, comparing, validating models (Chapters 5 6).practical guidance, recommend reading papers “Ten quick tips get started Bayesian statistics” (Gimenez et al. 2025) “Bayesian workflow” (Andrew Gelman et al. 2020).","code":""},{"path":"conclusions.html","id":"to-conclude","chapter":"Conclusions","heading":"To conclude","text":"Adopt pragmatic approach. choice statistical approach (frequentist Bayesian) depends objectives, whether concern speed, model complexity, type uncertainty want quantify. Discuss options experienced colleagues needed. Bayesian statistics dogma: powerful tool among others toolbox.Thank attention. Feel free write questions like see particular aspect developed new edition book. enjoy exploring Bayesian statistics!","code":""},{"path":"références.html","id":"références","chapter":"Références","heading":"Références","text":"","code":""}]
